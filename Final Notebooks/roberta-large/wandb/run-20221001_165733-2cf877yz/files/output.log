/home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
{'loss': 1.0561, 'learning_rate': 9.505000000000001e-06, 'epoch': 0.99}
{'eval_loss': 0.8772584795951843, 'eval_accuracy': 0.615, 'eval_precision': 0.4054487179487179, 'eval_recall': 0.5584415584415584, 'eval_f1': 0.45911330049261084, 'eval_runtime': 1.2922, 'eval_samples_per_second': 154.774, 'eval_steps_per_second': 10.06, 'epoch': 0.99}
***** Running Evaluation *****
  Num examples = 200
  Batch size = 16
/home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Saving model checkpoint to roberta-large-finetuned-ours-DS/checkpoint-99
Configuration saved in roberta-large-finetuned-ours-DS/checkpoint-99/config.json
Model weights saved in roberta-large-finetuned-ours-DS/checkpoint-99/pytorch_model.bin
tokenizer config file saved in roberta-large-finetuned-ours-DS/checkpoint-99/tokenizer_config.json
Special tokens file saved in roberta-large-finetuned-ours-DS/checkpoint-99/special_tokens_map.json
tokenizer config file saved in roberta-large-finetuned-ours-DS/tokenizer_config.json
Special tokens file saved in roberta-large-finetuned-ours-DS/special_tokens_map.json
/home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
{'loss': 0.762, 'learning_rate': 9.01e-06, 'epoch': 1.98}
{'eval_loss': 0.6514055132865906, 'eval_accuracy': 0.715, 'eval_precision': 0.6735453002006482, 'eval_recall': 0.6672097743526315, 'eval_f1': 0.6588466542225239, 'eval_runtime': 1.291, 'eval_samples_per_second': 154.919, 'eval_steps_per_second': 10.07, 'epoch': 1.98}
***** Running Evaluation *****
  Num examples = 200
  Batch size = 16
Saving model checkpoint to roberta-large-finetuned-ours-DS/checkpoint-198
Configuration saved in roberta-large-finetuned-ours-DS/checkpoint-198/config.json
Model weights saved in roberta-large-finetuned-ours-DS/checkpoint-198/pytorch_model.bin
tokenizer config file saved in roberta-large-finetuned-ours-DS/checkpoint-198/tokenizer_config.json
Special tokens file saved in roberta-large-finetuned-ours-DS/checkpoint-198/special_tokens_map.json
/home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
{'loss': 0.5661, 'learning_rate': 8.515e-06, 'epoch': 2.97}
***** Running Evaluation *****
  Num examples = 200
  Batch size = 16
{'eval_loss': 0.6806161403656006, 'eval_accuracy': 0.71, 'eval_precision': 0.6763580761745899, 'eval_recall': 0.660799517942375, 'eval_f1': 0.6434612488960315, 'eval_runtime': 1.2881, 'eval_samples_per_second': 155.263, 'eval_steps_per_second': 10.092, 'epoch': 2.97}
Saving model checkpoint to roberta-large-finetuned-ours-DS/checkpoint-297
Configuration saved in roberta-large-finetuned-ours-DS/checkpoint-297/config.json
Model weights saved in roberta-large-finetuned-ours-DS/checkpoint-297/pytorch_model.bin
tokenizer config file saved in roberta-large-finetuned-ours-DS/checkpoint-297/tokenizer_config.json
Special tokens file saved in roberta-large-finetuned-ours-DS/checkpoint-297/special_tokens_map.json
/home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
{'loss': 0.3699, 'learning_rate': 8.020000000000001e-06, 'epoch': 3.96}
***** Running Evaluation *****
  Num examples = 200
  Batch size = 16
{'eval_loss': 0.8357566595077515, 'eval_accuracy': 0.71, 'eval_precision': 0.6610927084611294, 'eval_recall': 0.6691443477157764, 'eval_f1': 0.6569987012837752, 'eval_runtime': 1.3179, 'eval_samples_per_second': 151.756, 'eval_steps_per_second': 9.864, 'epoch': 3.96}
Saving model checkpoint to roberta-large-finetuned-ours-DS/checkpoint-396
Configuration saved in roberta-large-finetuned-ours-DS/checkpoint-396/config.json
Model weights saved in roberta-large-finetuned-ours-DS/checkpoint-396/pytorch_model.bin
tokenizer config file saved in roberta-large-finetuned-ours-DS/checkpoint-396/tokenizer_config.json
Special tokens file saved in roberta-large-finetuned-ours-DS/checkpoint-396/special_tokens_map.json
Training completed. Do not forget to share your model on huggingface.co/models =)
Loading best model from roberta-large-finetuned-ours-DS/checkpoint-198 (score: 0.6588466542225239).
{'train_runtime': 184.1053, 'train_samples_per_second': 173.705, 'train_steps_per_second': 10.863, 'train_loss': 0.6885273191663954, 'epoch': 3.96}