{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LF0eI-2QE_ky"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHu-iZKrS6Tu"
      },
      "source": [
        "## 1) Installing libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "cFQX2EGiXgos"
      },
      "outputs": [],
      "source": [
        "# !pip install transformers\n",
        "# !pip install datasets\n",
        "# !pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "le8H055mTeqs"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "from datasets import load_dataset, Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0LlF1SVVvNM"
      },
      "source": [
        "## 2) Loading dataset (from HF)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "QPG3xAkTYsw7"
      },
      "outputs": [],
      "source": [
        "# enter your personal read token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "xdJCpTLOXiKv"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6403b58c32c64b60bf46b65066fa9fc4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "lTW-jsAmQesI"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using custom data configuration IIIT-L--total_code_mixed-c86de67ddd2696fa\n",
            "Reusing dataset csv (/home/diptesh/.cache/huggingface/datasets/IIIT-L___csv/IIIT-L--total_code_mixed-c86de67ddd2696fa/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58)\n"
          ]
        },
        {
          "data": {
            "application/json": {
              "ascii": false,
              "bar_format": null,
              "colour": null,
              "elapsed": 0.035719871520996094,
              "initial": 0,
              "n": 0,
              "ncols": null,
              "nrows": null,
              "postfix": null,
              "prefix": "",
              "rate": null,
              "total": 3,
              "unit": "it",
              "unit_divisor": 1000,
              "unit_scale": false
            },
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "37ac276cafad4897abd724b4a8888fdd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['Sentence', 'Label'],\n",
            "        num_rows: 3976\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['Sentence', 'Label'],\n",
            "        num_rows: 498\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['Sentence', 'Label'],\n",
            "        num_rows: 497\n",
            "    })\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "aggression_dataset = load_dataset(\"IIIT-L/total_code_mixed\", use_auth_token=True)\n",
        "\n",
        "print(aggression_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "43SsJM-aTlg7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['Sentence', 'Label'],\n",
              "    num_rows: 3976\n",
              "})"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_ds = aggression_dataset['train']\n",
        "train_ds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7T67guUEX0Nw"
      },
      "source": [
        "## 3) Converting to dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ZxPRh0hUUQz6"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentence</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>We don't spend one hour and rush off from our ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Shivani my classmate</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>True reviewerÃƒÂ°Ã…Â¸Ã¢â€žÂ¢Ã‚Â</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Royal C***yas Bangalore ÃƒÂ°Ã…Â¸Ã‹Å“Ã¢â‚¬Å¡ÃƒÂ...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>*PURE DESH ME AFRA TAFRI KA MAHOOL HAI...INDIA...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            Sentence  Label\n",
              "0  We don't spend one hour and rush off from our ...      1\n",
              "1                               Shivani my classmate      0\n",
              "2                   True reviewerÃƒÂ°Ã…Â¸Ã¢â€žÂ¢Ã‚Â      0\n",
              "3  Royal C***yas Bangalore ÃƒÂ°Ã…Â¸Ã‹Å“Ã¢â‚¬Å¡ÃƒÂ...      2\n",
              "4  *PURE DESH ME AFRA TAFRI KA MAHOOL HAI...INDIA...      2"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "aggression_dataset.set_format(type='pandas')\n",
        "train_df = aggression_dataset['train'][:]\n",
        "valid_df = aggression_dataset['validation'][:]\n",
        "\n",
        "train_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "IJsiywb_ofVt"
      },
      "outputs": [],
      "source": [
        "test_df = aggression_dataset['test'][:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "5LhCKCB4sMCa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0    2137\n",
              "1    1086\n",
              "2     753\n",
              "Name: Label, dtype: int64"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df['Label'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "bqe00WZ_IP2i"
      },
      "outputs": [],
      "source": [
        "# 3976\n",
        "# NAG-CAG-OAG (0-1-2) = 0.54-0.27-0.19"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFKTp8WlXubz"
      },
      "source": [
        "Seeing Distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "9tnlCy6dLRgi"
      },
      "outputs": [],
      "source": [
        "disb_df = train_df.copy(deep=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "wOdtcgKiXLZD"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages/matplotlib/cbook/__init__.py:1376: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  X = np.atleast_1d(X.T if isinstance(X, np.ndarray) else np.asarray(X))\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "disb_df['Words per sentence'] = disb_df['Sentence'].str.split().apply(len)\n",
        "disb_df.boxplot('Words per sentence', by='Label', grid=False, showfliers=False, color='black')\n",
        "plt.suptitle(\"\")\n",
        "plt.xlabel(\"\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5sA4SbW4jmYd"
      },
      "source": [
        "## 4) Tokenizing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "60uCbqkGjo0-"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "CwDB9kRGkG5L"
      },
      "outputs": [],
      "source": [
        "model_ckpt = 'roberta-large'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-iNzn8oleHo",
        "outputId": "0215f187-91cc-4de9-88f4-af75ecab1cd1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "50265"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.vocab_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "mk_bg4Pat9jw"
      },
      "outputs": [],
      "source": [
        "train_texts = list(train_df['Sentence'])\n",
        "train_labels = list(train_df['Label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "btVmfHrblxqm"
      },
      "outputs": [],
      "source": [
        "valid_texts = list(valid_df['Sentence'])\n",
        "valid_labels = list(valid_df['Label'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8RWWM8sMhyF"
      },
      "source": [
        "## 5) Encoding train-valid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "YV9Fz--nt8X_"
      },
      "outputs": [],
      "source": [
        "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=510)\n",
        "valid_encodings = tokenizer(valid_texts, truncation=True, padding=True, max_length=510)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Mc6Mpnqbuwx1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "class AggressionDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "mGql29l6ag6N"
      },
      "outputs": [],
      "source": [
        "train_dataset = AggressionDataset(train_encodings, train_labels)\n",
        "valid_dataset = AggressionDataset(valid_encodings, valid_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENs2HmKAanBd"
      },
      "source": [
        "## 6) Setting classification model and evaluation metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "DFmLAL7RbtPe"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "1JfA9ODa83rR"
      },
      "outputs": [],
      "source": [
        "# Use in case of CUDA memory error\n",
        "\n",
        "# import gc\n",
        "\n",
        "# gc.collect()\n",
        "# torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SwnXMX_Hap0V",
        "outputId": "596089c0-7061-4d83-8c0f-573804f42ea9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "num_labels = 3\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "\n",
        "def model_init():\n",
        "    model = (AutoModelForSequenceClassification.from_pretrained(model_ckpt, num_labels=num_labels))\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "IR3ZFBIjcF3H"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, plot_confusion_matrix\n",
        "\n",
        "def compute_metrics(pred):\n",
        "  labels = pred.label_ids\n",
        "  preds = pred.predictions.argmax(-1)\n",
        "\n",
        "  f1 = f1_score(labels, preds, average='macro')\n",
        "  precision = precision_score(labels, preds, average='macro')\n",
        "  recall = recall_score(labels, preds, average='macro')\n",
        "  acc = accuracy_score(labels, preds)\n",
        "  return {'accuracy': acc, 'precision': precision, 'recall': recall, 'f1': f1}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_1OptUlxh9-"
      },
      "source": [
        "## 7) Fine-tuning, visualizing training, saving model to HF  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "KGSxhrQ0vsfs"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdiptesh\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import wandb\n",
        "\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YtVAykzCv7vZ",
        "outputId": "128d694c-7f5c-4e87-82f1-5518427c2ec4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "env: WANDB_PROJECT=aggression_detection\n"
          ]
        }
      ],
      "source": [
        "%env WANDB_PROJECT = aggression_detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "EDakmiAHc150"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer, TrainingArguments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "_LlQYDjndBFG"
      },
      "outputs": [],
      "source": [
        "# Defining hyperparameters\n",
        "eval_batch_size = 16\n",
        "logging_steps = len(train_texts) // eval_batch_size\n",
        "model_name = f\"{model_ckpt}-finetuned-code-mixed-DS\"\n",
        "training_args = TrainingArguments(output_dir=model_name,\n",
        "                                  num_train_epochs=20,\n",
        "                                  learning_rate=1e-05,\n",
        "                                  per_device_train_batch_size=8,\n",
        "                                  per_device_eval_batch_size=16,\n",
        "                                  weight_decay=0.01,\n",
        "                                  evaluation_strategy='steps',\n",
        "                                  save_strategy='steps',\n",
        "                                  max_steps=-1,\n",
        "                                  warmup_ratio=0.0,\n",
        "                                  seed=43,\n",
        "                                  data_seed=4,\n",
        "                                  metric_for_best_model=\"eval_f1\",\n",
        "                                  greater_is_better=True,\n",
        "                                  load_best_model_at_end=True, \n",
        "                                  disable_tqdm=False,\n",
        "                                  logging_steps=logging_steps,\n",
        "                                  save_steps=logging_steps,\n",
        "                                  log_level='info', \n",
        "                                  report_to=\"wandb\", \n",
        "                                  run_name=\"roberta-large-code-mixed-DS\",\n",
        "                                  push_to_hub=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "bC3nDg818V3U"
      },
      "outputs": [],
      "source": [
        "# import gc\n",
        "\n",
        "# gc.collect()\n",
        "# torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "Moy_vPC1XsQ7"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "\n",
        "class CustomTrainer(Trainer):\n",
        "  def compute_loss(self, model, inputs, return_outputs=False):\n",
        "    # device = torch.device('cuda')\n",
        "    # inputs.to(device)\n",
        "    labels = inputs.get(\"labels\")\n",
        "    # forward pass\n",
        "    outputs = model(**inputs)\n",
        "    logits = outputs.get(\"logits\")\n",
        "    # compute custom loss (suppose one has 3 labels with different weights)\n",
        "    loss_fct = nn.CrossEntropyLoss(weight=torch.tensor([0.17, 0.27, 0.56]).to(device))\n",
        "    loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
        "    return (loss, outputs) if return_outputs else loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "a-a-65FPl5YL"
      },
      "outputs": [],
      "source": [
        "from transformers import EarlyStoppingCallback"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "KJDDBeXSoSf5"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "12855d56fea7453aa0796b20cbcb305e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# enter your personal write token here\n",
        "\n",
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "bgj9CC3qeD22"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loading configuration file https://huggingface.co/roberta-large/resolve/main/config.json from cache at /home/diptesh/.cache/huggingface/transformers/dea67b44b38d504f2523f3ddb6acb601b23d67bee52c942da336fa1283100990.94cae8b3a8dbab1d59b9d4827f7ce79e73124efa6bb970412cd503383a95f373\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"roberta-large\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading weights file https://huggingface.co/roberta-large/resolve/main/pytorch_model.bin from cache at /home/diptesh/.cache/huggingface/transformers/8e36ec2f5052bec1e79e139b84c2c3089cb647694ba0f4f634fec7b8258f7c89.c43841d8c5cd23c435408295164cda9525270aa42cd0cc9200911570c0342352\n",
            "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/home/diptesh/workspace/AggressionDetection-IIITL/Final Notebooks/roberta-large/roberta-large-finetuned-code-mixed-DS is already a clone of https://huggingface.co/dipteshkanojia/roberta-large-finetuned-code-mixed-DS. Make sure you pull the latest changes with `repo.git_pull()`.\n",
            "loading configuration file https://huggingface.co/roberta-large/resolve/main/config.json from cache at /home/diptesh/.cache/huggingface/transformers/dea67b44b38d504f2523f3ddb6acb601b23d67bee52c942da336fa1283100990.94cae8b3a8dbab1d59b9d4827f7ce79e73124efa6bb970412cd503383a95f373\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"roberta-large\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading weights file https://huggingface.co/roberta-large/resolve/main/pytorch_model.bin from cache at /home/diptesh/.cache/huggingface/transformers/8e36ec2f5052bec1e79e139b84c2c3089cb647694ba0f4f634fec7b8258f7c89.c43841d8c5cd23c435408295164cda9525270aa42cd0cc9200911570c0342352\n",
            "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 3976\n",
            "  Num Epochs = 20\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 4980\n",
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href=\"https://wandb.me/wandb-init\" target=\"_blank\">the W&B docs</a>."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.13.3 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.13.2"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/diptesh/workspace/AggressionDetection-IIITL/Final Notebooks/roberta-large/wandb/run-20221001_231608-tcobsn8r</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/diptesh/aggression_detection/runs/tcobsn8r\" target=\"_blank\">roberta-large-code-mixed-DS</a></strong> to <a href=\"https://wandb.ai/diptesh/aggression_detection\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/json": {
              "ascii": false,
              "bar_format": null,
              "colour": null,
              "elapsed": 0.03236126899719238,
              "initial": 0,
              "n": 0,
              "ncols": null,
              "nrows": null,
              "postfix": null,
              "prefix": "",
              "rate": null,
              "total": 4980,
              "unit": "it",
              "unit_divisor": 1000,
              "unit_scale": false
            },
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1dac3989ccae427fb3494ac553435160",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/4980 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 497\n",
            "  Batch size = 32\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.9729, 'learning_rate': 9.502008032128515e-06, 'epoch': 1.0}\n"
          ]
        },
        {
          "data": {
            "application/json": {
              "ascii": false,
              "bar_format": null,
              "colour": null,
              "elapsed": 0.014524221420288086,
              "initial": 0,
              "n": 0,
              "ncols": null,
              "nrows": null,
              "postfix": null,
              "prefix": "",
              "rate": null,
              "total": 16,
              "unit": "it",
              "unit_divisor": 1000,
              "unit_scale": false
            },
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "721622c876ae44058c1eaeb432185c17",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/16 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to roberta-large-finetuned-code-mixed-DS/checkpoint-248\n",
            "Configuration saved in roberta-large-finetuned-code-mixed-DS/checkpoint-248/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.749129593372345, 'eval_accuracy': 0.6921529175050302, 'eval_precision': 0.6433783597926443, 'eval_recall': 0.6624518163309631, 'eval_f1': 0.6357616649992619, 'eval_runtime': 5.2031, 'eval_samples_per_second': 95.519, 'eval_steps_per_second': 3.075, 'epoch': 1.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in roberta-large-finetuned-code-mixed-DS/checkpoint-248/pytorch_model.bin\n",
            "tokenizer config file saved in roberta-large-finetuned-code-mixed-DS/checkpoint-248/tokenizer_config.json\n",
            "Special tokens file saved in roberta-large-finetuned-code-mixed-DS/checkpoint-248/special_tokens_map.json\n",
            "tokenizer config file saved in roberta-large-finetuned-code-mixed-DS/tokenizer_config.json\n",
            "Special tokens file saved in roberta-large-finetuned-code-mixed-DS/special_tokens_map.json\n",
            "/home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 497\n",
            "  Batch size = 32\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.7474, 'learning_rate': 9.004016064257028e-06, 'epoch': 1.99}\n"
          ]
        },
        {
          "data": {
            "application/json": {
              "ascii": false,
              "bar_format": null,
              "colour": null,
              "elapsed": 0.015237808227539062,
              "initial": 0,
              "n": 0,
              "ncols": null,
              "nrows": null,
              "postfix": null,
              "prefix": "",
              "rate": null,
              "total": 16,
              "unit": "it",
              "unit_divisor": 1000,
              "unit_scale": false
            },
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "196d444b00cb478b83e45c6dbd6fd589",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/16 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to roberta-large-finetuned-code-mixed-DS/checkpoint-496\n",
            "Configuration saved in roberta-large-finetuned-code-mixed-DS/checkpoint-496/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.6947492957115173, 'eval_accuracy': 0.7183098591549296, 'eval_precision': 0.6712434518886132, 'eval_recall': 0.6914588929045203, 'eval_f1': 0.6760448407728324, 'eval_runtime': 5.2063, 'eval_samples_per_second': 95.462, 'eval_steps_per_second': 3.073, 'epoch': 1.99}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in roberta-large-finetuned-code-mixed-DS/checkpoint-496/pytorch_model.bin\n",
            "tokenizer config file saved in roberta-large-finetuned-code-mixed-DS/checkpoint-496/tokenizer_config.json\n",
            "Special tokens file saved in roberta-large-finetuned-code-mixed-DS/checkpoint-496/special_tokens_map.json\n",
            "/home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 497\n",
            "  Batch size = 32\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.5938, 'learning_rate': 8.506024096385543e-06, 'epoch': 2.99}\n"
          ]
        },
        {
          "data": {
            "application/json": {
              "ascii": false,
              "bar_format": null,
              "colour": null,
              "elapsed": 0.014466524124145508,
              "initial": 0,
              "n": 0,
              "ncols": null,
              "nrows": null,
              "postfix": null,
              "prefix": "",
              "rate": null,
              "total": 16,
              "unit": "it",
              "unit_divisor": 1000,
              "unit_scale": false
            },
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "125bca62fbd44daca82e1d8f0c32c63f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/16 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to roberta-large-finetuned-code-mixed-DS/checkpoint-744\n",
            "Configuration saved in roberta-large-finetuned-code-mixed-DS/checkpoint-744/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.7370456457138062, 'eval_accuracy': 0.7122736418511066, 'eval_precision': 0.6623525322537774, 'eval_recall': 0.6838911076423556, 'eval_f1': 0.6641636071820569, 'eval_runtime': 5.2223, 'eval_samples_per_second': 95.169, 'eval_steps_per_second': 3.064, 'epoch': 2.99}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in roberta-large-finetuned-code-mixed-DS/checkpoint-744/pytorch_model.bin\n",
            "tokenizer config file saved in roberta-large-finetuned-code-mixed-DS/checkpoint-744/tokenizer_config.json\n",
            "Special tokens file saved in roberta-large-finetuned-code-mixed-DS/checkpoint-744/special_tokens_map.json\n",
            "tokenizer config file saved in roberta-large-finetuned-code-mixed-DS/tokenizer_config.json\n",
            "Special tokens file saved in roberta-large-finetuned-code-mixed-DS/special_tokens_map.json\n",
            "/home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 497\n",
            "  Batch size = 32\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.4264, 'learning_rate': 8.008032128514057e-06, 'epoch': 3.98}\n"
          ]
        },
        {
          "data": {
            "application/json": {
              "ascii": false,
              "bar_format": null,
              "colour": null,
              "elapsed": 0.0180666446685791,
              "initial": 0,
              "n": 0,
              "ncols": null,
              "nrows": null,
              "postfix": null,
              "prefix": "",
              "rate": null,
              "total": 16,
              "unit": "it",
              "unit_divisor": 1000,
              "unit_scale": false
            },
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c43b8cb810bc4a60ab1a9d6b3ae78446",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/16 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to roberta-large-finetuned-code-mixed-DS/checkpoint-992\n",
            "Configuration saved in roberta-large-finetuned-code-mixed-DS/checkpoint-992/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.8820438385009766, 'eval_accuracy': 0.7122736418511066, 'eval_precision': 0.6539948485828885, 'eval_recall': 0.6635553336802089, 'eval_f1': 0.6491535871759324, 'eval_runtime': 5.4138, 'eval_samples_per_second': 91.803, 'eval_steps_per_second': 2.955, 'epoch': 3.98}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in roberta-large-finetuned-code-mixed-DS/checkpoint-992/pytorch_model.bin\n",
            "tokenizer config file saved in roberta-large-finetuned-code-mixed-DS/checkpoint-992/tokenizer_config.json\n",
            "Special tokens file saved in roberta-large-finetuned-code-mixed-DS/checkpoint-992/special_tokens_map.json\n",
            "/home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 497\n",
            "  Batch size = 32\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.2806, 'learning_rate': 7.510040160642571e-06, 'epoch': 4.98}\n"
          ]
        },
        {
          "data": {
            "application/json": {
              "ascii": false,
              "bar_format": null,
              "colour": null,
              "elapsed": 0.014478445053100586,
              "initial": 0,
              "n": 0,
              "ncols": null,
              "nrows": null,
              "postfix": null,
              "prefix": "",
              "rate": null,
              "total": 16,
              "unit": "it",
              "unit_divisor": 1000,
              "unit_scale": false
            },
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9debcb719f284e098cd3a2d1f7fdd330",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/16 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to roberta-large-finetuned-code-mixed-DS/checkpoint-1240\n",
            "Configuration saved in roberta-large-finetuned-code-mixed-DS/checkpoint-1240/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 1.2021782398223877, 'eval_accuracy': 0.7404426559356136, 'eval_precision': 0.6806882629969732, 'eval_recall': 0.6694168662763537, 'eval_f1': 0.6742183490303808, 'eval_runtime': 5.345, 'eval_samples_per_second': 92.984, 'eval_steps_per_second': 2.993, 'epoch': 4.98}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in roberta-large-finetuned-code-mixed-DS/checkpoint-1240/pytorch_model.bin\n",
            "tokenizer config file saved in roberta-large-finetuned-code-mixed-DS/checkpoint-1240/tokenizer_config.json\n",
            "Special tokens file saved in roberta-large-finetuned-code-mixed-DS/checkpoint-1240/special_tokens_map.json\n",
            "tokenizer config file saved in roberta-large-finetuned-code-mixed-DS/tokenizer_config.json\n",
            "Special tokens file saved in roberta-large-finetuned-code-mixed-DS/special_tokens_map.json\n",
            "/home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 497\n",
            "  Batch size = 32\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.2239, 'learning_rate': 7.012048192771085e-06, 'epoch': 5.98}\n"
          ]
        },
        {
          "data": {
            "application/json": {
              "ascii": false,
              "bar_format": null,
              "colour": null,
              "elapsed": 0.014971017837524414,
              "initial": 0,
              "n": 0,
              "ncols": null,
              "nrows": null,
              "postfix": null,
              "prefix": "",
              "rate": null,
              "total": 16,
              "unit": "it",
              "unit_divisor": 1000,
              "unit_scale": false
            },
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8d3f6e1f12f540b6b0efda72c474c3e4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/16 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to roberta-large-finetuned-code-mixed-DS/checkpoint-1488\n",
            "Configuration saved in roberta-large-finetuned-code-mixed-DS/checkpoint-1488/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 1.393298864364624, 'eval_accuracy': 0.7223340040241448, 'eval_precision': 0.65930526800092, 'eval_recall': 0.6587180214344085, 'eval_f1': 0.6567916979469014, 'eval_runtime': 5.2912, 'eval_samples_per_second': 93.93, 'eval_steps_per_second': 3.024, 'epoch': 5.98}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in roberta-large-finetuned-code-mixed-DS/checkpoint-1488/pytorch_model.bin\n",
            "tokenizer config file saved in roberta-large-finetuned-code-mixed-DS/checkpoint-1488/tokenizer_config.json\n",
            "Special tokens file saved in roberta-large-finetuned-code-mixed-DS/checkpoint-1488/special_tokens_map.json\n",
            "/home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 497\n",
            "  Batch size = 32\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.1585, 'learning_rate': 6.514056224899599e-06, 'epoch': 6.97}\n"
          ]
        },
        {
          "data": {
            "application/json": {
              "ascii": false,
              "bar_format": null,
              "colour": null,
              "elapsed": 0.014468669891357422,
              "initial": 0,
              "n": 0,
              "ncols": null,
              "nrows": null,
              "postfix": null,
              "prefix": "",
              "rate": null,
              "total": 16,
              "unit": "it",
              "unit_divisor": 1000,
              "unit_scale": false
            },
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "df3fecdbddb24717ad8256017d3c335b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/16 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to roberta-large-finetuned-code-mixed-DS/checkpoint-1736\n",
            "Configuration saved in roberta-large-finetuned-code-mixed-DS/checkpoint-1736/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 1.854324460029602, 'eval_accuracy': 0.7303822937625755, 'eval_precision': 0.6730362561564608, 'eval_recall': 0.6762951973362458, 'eval_f1': 0.6736900728250209, 'eval_runtime': 5.2514, 'eval_samples_per_second': 94.642, 'eval_steps_per_second': 3.047, 'epoch': 6.97}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in roberta-large-finetuned-code-mixed-DS/checkpoint-1736/pytorch_model.bin\n",
            "tokenizer config file saved in roberta-large-finetuned-code-mixed-DS/checkpoint-1736/tokenizer_config.json\n",
            "Special tokens file saved in roberta-large-finetuned-code-mixed-DS/checkpoint-1736/special_tokens_map.json\n",
            "tokenizer config file saved in roberta-large-finetuned-code-mixed-DS/tokenizer_config.json\n",
            "Special tokens file saved in roberta-large-finetuned-code-mixed-DS/special_tokens_map.json\n",
            "/home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 497\n",
            "  Batch size = 32\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.1302, 'learning_rate': 6.016064257028112e-06, 'epoch': 7.97}\n"
          ]
        },
        {
          "data": {
            "application/json": {
              "ascii": false,
              "bar_format": null,
              "colour": null,
              "elapsed": 0.01483297348022461,
              "initial": 0,
              "n": 0,
              "ncols": null,
              "nrows": null,
              "postfix": null,
              "prefix": "",
              "rate": null,
              "total": 16,
              "unit": "it",
              "unit_divisor": 1000,
              "unit_scale": false
            },
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "652571bb5a554e64943b028d53b19e3f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/16 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to roberta-large-finetuned-code-mixed-DS/checkpoint-1984\n",
            "Configuration saved in roberta-large-finetuned-code-mixed-DS/checkpoint-1984/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 2.0783028602600098, 'eval_accuracy': 0.7142857142857143, 'eval_precision': 0.6495330777102493, 'eval_recall': 0.6519846124759571, 'eval_f1': 0.6504159673722772, 'eval_runtime': 5.5193, 'eval_samples_per_second': 90.047, 'eval_steps_per_second': 2.899, 'epoch': 7.97}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in roberta-large-finetuned-code-mixed-DS/checkpoint-1984/pytorch_model.bin\n",
            "tokenizer config file saved in roberta-large-finetuned-code-mixed-DS/checkpoint-1984/tokenizer_config.json\n",
            "Special tokens file saved in roberta-large-finetuned-code-mixed-DS/checkpoint-1984/special_tokens_map.json\n",
            "/home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 497\n",
            "  Batch size = 32\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.1008, 'learning_rate': 5.518072289156628e-06, 'epoch': 8.96}\n"
          ]
        },
        {
          "data": {
            "application/json": {
              "ascii": false,
              "bar_format": null,
              "colour": null,
              "elapsed": 0.014478206634521484,
              "initial": 0,
              "n": 0,
              "ncols": null,
              "nrows": null,
              "postfix": null,
              "prefix": "",
              "rate": null,
              "total": 16,
              "unit": "it",
              "unit_divisor": 1000,
              "unit_scale": false
            },
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b21d99faada54f1d88cdafaacbac384c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/16 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to roberta-large-finetuned-code-mixed-DS/checkpoint-2232\n",
            "Configuration saved in roberta-large-finetuned-code-mixed-DS/checkpoint-2232/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 2.3523097038269043, 'eval_accuracy': 0.7183098591549296, 'eval_precision': 0.6587955435519605, 'eval_recall': 0.6561137204901882, 'eval_f1': 0.6551644944300312, 'eval_runtime': 5.5091, 'eval_samples_per_second': 90.214, 'eval_steps_per_second': 2.904, 'epoch': 8.96}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in roberta-large-finetuned-code-mixed-DS/checkpoint-2232/pytorch_model.bin\n",
            "tokenizer config file saved in roberta-large-finetuned-code-mixed-DS/checkpoint-2232/tokenizer_config.json\n",
            "Special tokens file saved in roberta-large-finetuned-code-mixed-DS/checkpoint-2232/special_tokens_map.json\n",
            "tokenizer config file saved in roberta-large-finetuned-code-mixed-DS/tokenizer_config.json\n",
            "Special tokens file saved in roberta-large-finetuned-code-mixed-DS/special_tokens_map.json\n",
            "/home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 497\n",
            "  Batch size = 32\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0793, 'learning_rate': 5.0200803212851415e-06, 'epoch': 9.96}\n"
          ]
        },
        {
          "data": {
            "application/json": {
              "ascii": false,
              "bar_format": null,
              "colour": null,
              "elapsed": 0.015217781066894531,
              "initial": 0,
              "n": 0,
              "ncols": null,
              "nrows": null,
              "postfix": null,
              "prefix": "",
              "rate": null,
              "total": 16,
              "unit": "it",
              "unit_divisor": 1000,
              "unit_scale": false
            },
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4588c62fb77c4f578224e6dba424299e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/16 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to roberta-large-finetuned-code-mixed-DS/checkpoint-2480\n",
            "Configuration saved in roberta-large-finetuned-code-mixed-DS/checkpoint-2480/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 2.5260074138641357, 'eval_accuracy': 0.716297786720322, 'eval_precision': 0.6516042116671308, 'eval_recall': 0.6566258306653604, 'eval_f1': 0.6538299803623703, 'eval_runtime': 5.5099, 'eval_samples_per_second': 90.201, 'eval_steps_per_second': 2.904, 'epoch': 9.96}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in roberta-large-finetuned-code-mixed-DS/checkpoint-2480/pytorch_model.bin\n",
            "tokenizer config file saved in roberta-large-finetuned-code-mixed-DS/checkpoint-2480/tokenizer_config.json\n",
            "Special tokens file saved in roberta-large-finetuned-code-mixed-DS/checkpoint-2480/special_tokens_map.json\n",
            "/home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 497\n",
            "  Batch size = 32\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0498, 'learning_rate': 4.522088353413655e-06, 'epoch': 10.96}\n"
          ]
        },
        {
          "data": {
            "application/json": {
              "ascii": false,
              "bar_format": null,
              "colour": null,
              "elapsed": 0.014613866806030273,
              "initial": 0,
              "n": 0,
              "ncols": null,
              "nrows": null,
              "postfix": null,
              "prefix": "",
              "rate": null,
              "total": 16,
              "unit": "it",
              "unit_divisor": 1000,
              "unit_scale": false
            },
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8c96b68ec2ec4c15aa74a9b9f4fb4d8a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/16 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to roberta-large-finetuned-code-mixed-DS/checkpoint-2728\n",
            "Configuration saved in roberta-large-finetuned-code-mixed-DS/checkpoint-2728/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 2.6074488162994385, 'eval_accuracy': 0.7424547283702213, 'eval_precision': 0.6902273826802129, 'eval_recall': 0.6817030182859661, 'eval_f1': 0.6830249686050625, 'eval_runtime': 5.333, 'eval_samples_per_second': 93.193, 'eval_steps_per_second': 3.0, 'epoch': 10.96}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in roberta-large-finetuned-code-mixed-DS/checkpoint-2728/pytorch_model.bin\n",
            "tokenizer config file saved in roberta-large-finetuned-code-mixed-DS/checkpoint-2728/tokenizer_config.json\n",
            "Special tokens file saved in roberta-large-finetuned-code-mixed-DS/checkpoint-2728/special_tokens_map.json\n",
            "tokenizer config file saved in roberta-large-finetuned-code-mixed-DS/tokenizer_config.json\n",
            "Special tokens file saved in roberta-large-finetuned-code-mixed-DS/special_tokens_map.json\n",
            "/home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 497\n",
            "  Batch size = 32\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0484, 'learning_rate': 4.024096385542169e-06, 'epoch': 11.95}\n"
          ]
        },
        {
          "data": {
            "application/json": {
              "ascii": false,
              "bar_format": null,
              "colour": null,
              "elapsed": 0.014679908752441406,
              "initial": 0,
              "n": 0,
              "ncols": null,
              "nrows": null,
              "postfix": null,
              "prefix": "",
              "rate": null,
              "total": 16,
              "unit": "it",
              "unit_divisor": 1000,
              "unit_scale": false
            },
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ab72b9c609ec4fad8f71523fb5f06584",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/16 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to roberta-large-finetuned-code-mixed-DS/checkpoint-2976\n",
            "Configuration saved in roberta-large-finetuned-code-mixed-DS/checkpoint-2976/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 2.6758031845092773, 'eval_accuracy': 0.7283702213279678, 'eval_precision': 0.6686799043941901, 'eval_recall': 0.6734145287727012, 'eval_f1': 0.6708581308205214, 'eval_runtime': 5.2592, 'eval_samples_per_second': 94.501, 'eval_steps_per_second': 3.042, 'epoch': 11.95}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in roberta-large-finetuned-code-mixed-DS/checkpoint-2976/pytorch_model.bin\n",
            "tokenizer config file saved in roberta-large-finetuned-code-mixed-DS/checkpoint-2976/tokenizer_config.json\n",
            "Special tokens file saved in roberta-large-finetuned-code-mixed-DS/checkpoint-2976/special_tokens_map.json\n",
            "/home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 497\n",
            "  Batch size = 32\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0409, 'learning_rate': 3.526104417670683e-06, 'epoch': 12.95}\n"
          ]
        },
        {
          "data": {
            "application/json": {
              "ascii": false,
              "bar_format": null,
              "colour": null,
              "elapsed": 0.014539718627929688,
              "initial": 0,
              "n": 0,
              "ncols": null,
              "nrows": null,
              "postfix": null,
              "prefix": "",
              "rate": null,
              "total": 16,
              "unit": "it",
              "unit_divisor": 1000,
              "unit_scale": false
            },
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8eeeb35ae92d4d59987ead7dc90663dd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/16 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to roberta-large-finetuned-code-mixed-DS/checkpoint-3224\n",
            "Configuration saved in roberta-large-finetuned-code-mixed-DS/checkpoint-3224/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 2.8657548427581787, 'eval_accuracy': 0.7424547283702213, 'eval_precision': 0.6817319312116776, 'eval_recall': 0.6755828915357679, 'eval_f1': 0.6781200565879284, 'eval_runtime': 5.2943, 'eval_samples_per_second': 93.874, 'eval_steps_per_second': 3.022, 'epoch': 12.95}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in roberta-large-finetuned-code-mixed-DS/checkpoint-3224/pytorch_model.bin\n",
            "tokenizer config file saved in roberta-large-finetuned-code-mixed-DS/checkpoint-3224/tokenizer_config.json\n",
            "Special tokens file saved in roberta-large-finetuned-code-mixed-DS/checkpoint-3224/special_tokens_map.json\n",
            "tokenizer config file saved in roberta-large-finetuned-code-mixed-DS/tokenizer_config.json\n",
            "Special tokens file saved in roberta-large-finetuned-code-mixed-DS/special_tokens_map.json\n",
            "/home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 497\n",
            "  Batch size = 32\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0239, 'learning_rate': 3.028112449799197e-06, 'epoch': 13.94}\n"
          ]
        },
        {
          "data": {
            "application/json": {
              "ascii": false,
              "bar_format": null,
              "colour": null,
              "elapsed": 0.02253556251525879,
              "initial": 0,
              "n": 0,
              "ncols": null,
              "nrows": null,
              "postfix": null,
              "prefix": "",
              "rate": null,
              "total": 16,
              "unit": "it",
              "unit_divisor": 1000,
              "unit_scale": false
            },
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dd6272eaf33649bca643e87844cd0111",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/16 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to roberta-large-finetuned-code-mixed-DS/checkpoint-3472\n",
            "Configuration saved in roberta-large-finetuned-code-mixed-DS/checkpoint-3472/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 2.9484469890594482, 'eval_accuracy': 0.7464788732394366, 'eval_precision': 0.6979777269710157, 'eval_recall': 0.681794815304399, 'eval_f1': 0.6870287706994859, 'eval_runtime': 5.3245, 'eval_samples_per_second': 93.341, 'eval_steps_per_second': 3.005, 'epoch': 13.94}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in roberta-large-finetuned-code-mixed-DS/checkpoint-3472/pytorch_model.bin\n",
            "tokenizer config file saved in roberta-large-finetuned-code-mixed-DS/checkpoint-3472/tokenizer_config.json\n",
            "Special tokens file saved in roberta-large-finetuned-code-mixed-DS/checkpoint-3472/special_tokens_map.json\n",
            "/home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 497\n",
            "  Batch size = 32\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.025, 'learning_rate': 2.530120481927711e-06, 'epoch': 14.94}\n"
          ]
        },
        {
          "data": {
            "application/json": {
              "ascii": false,
              "bar_format": null,
              "colour": null,
              "elapsed": 0.017228126525878906,
              "initial": 0,
              "n": 0,
              "ncols": null,
              "nrows": null,
              "postfix": null,
              "prefix": "",
              "rate": null,
              "total": 16,
              "unit": "it",
              "unit_divisor": 1000,
              "unit_scale": false
            },
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "499de966fde24f52a12a320ed7a7175f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/16 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to roberta-large-finetuned-code-mixed-DS/checkpoint-3720\n",
            "Configuration saved in roberta-large-finetuned-code-mixed-DS/checkpoint-3720/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 3.0827226638793945, 'eval_accuracy': 0.7303822937625755, 'eval_precision': 0.6777824756164107, 'eval_recall': 0.657699074529804, 'eval_f1': 0.6641239772678119, 'eval_runtime': 5.291, 'eval_samples_per_second': 93.934, 'eval_steps_per_second': 3.024, 'epoch': 14.94}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in roberta-large-finetuned-code-mixed-DS/checkpoint-3720/pytorch_model.bin\n",
            "tokenizer config file saved in roberta-large-finetuned-code-mixed-DS/checkpoint-3720/tokenizer_config.json\n",
            "Special tokens file saved in roberta-large-finetuned-code-mixed-DS/checkpoint-3720/special_tokens_map.json\n",
            "tokenizer config file saved in roberta-large-finetuned-code-mixed-DS/tokenizer_config.json\n",
            "Special tokens file saved in roberta-large-finetuned-code-mixed-DS/special_tokens_map.json\n",
            "/home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 497\n",
            "  Batch size = 32\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0286, 'learning_rate': 2.032128514056225e-06, 'epoch': 15.94}\n"
          ]
        },
        {
          "data": {
            "application/json": {
              "ascii": false,
              "bar_format": null,
              "colour": null,
              "elapsed": 0.01451730728149414,
              "initial": 0,
              "n": 0,
              "ncols": null,
              "nrows": null,
              "postfix": null,
              "prefix": "",
              "rate": null,
              "total": 16,
              "unit": "it",
              "unit_divisor": 1000,
              "unit_scale": false
            },
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "747064d81efe4f88a0c1de6e78a1ce80",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/16 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to roberta-large-finetuned-code-mixed-DS/checkpoint-3968\n",
            "Configuration saved in roberta-large-finetuned-code-mixed-DS/checkpoint-3968/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 3.0011253356933594, 'eval_accuracy': 0.7183098591549296, 'eval_precision': 0.6509221772379666, 'eval_recall': 0.6474810898142028, 'eval_f1': 0.6490976726416626, 'eval_runtime': 5.2621, 'eval_samples_per_second': 94.449, 'eval_steps_per_second': 3.041, 'epoch': 15.94}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in roberta-large-finetuned-code-mixed-DS/checkpoint-3968/pytorch_model.bin\n",
            "tokenizer config file saved in roberta-large-finetuned-code-mixed-DS/checkpoint-3968/tokenizer_config.json\n",
            "Special tokens file saved in roberta-large-finetuned-code-mixed-DS/checkpoint-3968/special_tokens_map.json\n",
            "/home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 497\n",
            "  Batch size = 32\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0264, 'learning_rate': 1.534136546184739e-06, 'epoch': 16.93}\n"
          ]
        },
        {
          "data": {
            "application/json": {
              "ascii": false,
              "bar_format": null,
              "colour": null,
              "elapsed": 0.014964103698730469,
              "initial": 0,
              "n": 0,
              "ncols": null,
              "nrows": null,
              "postfix": null,
              "prefix": "",
              "rate": null,
              "total": 16,
              "unit": "it",
              "unit_divisor": 1000,
              "unit_scale": false
            },
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8f23983b7d374bcdb256f2c0d884273c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/16 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to roberta-large-finetuned-code-mixed-DS/checkpoint-4216\n",
            "Configuration saved in roberta-large-finetuned-code-mixed-DS/checkpoint-4216/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 3.1580655574798584, 'eval_accuracy': 0.7263581488933601, 'eval_precision': 0.6645086277514997, 'eval_recall': 0.656297314527054, 'eval_f1': 0.6594766610953662, 'eval_runtime': 5.3083, 'eval_samples_per_second': 93.627, 'eval_steps_per_second': 3.014, 'epoch': 16.93}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in roberta-large-finetuned-code-mixed-DS/checkpoint-4216/pytorch_model.bin\n",
            "tokenizer config file saved in roberta-large-finetuned-code-mixed-DS/checkpoint-4216/tokenizer_config.json\n",
            "Special tokens file saved in roberta-large-finetuned-code-mixed-DS/checkpoint-4216/special_tokens_map.json\n",
            "tokenizer config file saved in roberta-large-finetuned-code-mixed-DS/tokenizer_config.json\n",
            "Special tokens file saved in roberta-large-finetuned-code-mixed-DS/special_tokens_map.json\n",
            "/home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 497\n",
            "  Batch size = 32\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.009, 'learning_rate': 1.0361445783132532e-06, 'epoch': 17.93}\n"
          ]
        },
        {
          "data": {
            "application/json": {
              "ascii": false,
              "bar_format": null,
              "colour": null,
              "elapsed": 0.014807939529418945,
              "initial": 0,
              "n": 0,
              "ncols": null,
              "nrows": null,
              "postfix": null,
              "prefix": "",
              "rate": null,
              "total": 16,
              "unit": "it",
              "unit_divisor": 1000,
              "unit_scale": false
            },
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bb9acecdb79b42a6920f80370ef98075",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/16 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to roberta-large-finetuned-code-mixed-DS/checkpoint-4464\n",
            "Configuration saved in roberta-large-finetuned-code-mixed-DS/checkpoint-4464/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 3.120044469833374, 'eval_accuracy': 0.7223340040241448, 'eval_precision': 0.6589171256945344, 'eval_recall': 0.6560980954657741, 'eval_f1': 0.6568582956796408, 'eval_runtime': 5.3099, 'eval_samples_per_second': 93.599, 'eval_steps_per_second': 3.013, 'epoch': 17.93}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in roberta-large-finetuned-code-mixed-DS/checkpoint-4464/pytorch_model.bin\n",
            "tokenizer config file saved in roberta-large-finetuned-code-mixed-DS/checkpoint-4464/tokenizer_config.json\n",
            "Special tokens file saved in roberta-large-finetuned-code-mixed-DS/checkpoint-4464/special_tokens_map.json\n",
            "/home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 497\n",
            "  Batch size = 32\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.012, 'learning_rate': 5.381526104417671e-07, 'epoch': 18.92}\n"
          ]
        },
        {
          "data": {
            "application/json": {
              "ascii": false,
              "bar_format": null,
              "colour": null,
              "elapsed": 0.014549493789672852,
              "initial": 0,
              "n": 0,
              "ncols": null,
              "nrows": null,
              "postfix": null,
              "prefix": "",
              "rate": null,
              "total": 16,
              "unit": "it",
              "unit_divisor": 1000,
              "unit_scale": false
            },
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f48392fb4cc14e81b2f7d5f182ad1de5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/16 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to roberta-large-finetuned-code-mixed-DS/checkpoint-4712\n",
            "Configuration saved in roberta-large-finetuned-code-mixed-DS/checkpoint-4712/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 3.136431932449341, 'eval_accuracy': 0.7203219315895373, 'eval_precision': 0.6573372709359442, 'eval_recall': 0.6502543363349006, 'eval_f1': 0.652507819502686, 'eval_runtime': 5.24, 'eval_samples_per_second': 94.847, 'eval_steps_per_second': 3.053, 'epoch': 18.92}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in roberta-large-finetuned-code-mixed-DS/checkpoint-4712/pytorch_model.bin\n",
            "tokenizer config file saved in roberta-large-finetuned-code-mixed-DS/checkpoint-4712/tokenizer_config.json\n",
            "Special tokens file saved in roberta-large-finetuned-code-mixed-DS/checkpoint-4712/special_tokens_map.json\n",
            "tokenizer config file saved in roberta-large-finetuned-code-mixed-DS/tokenizer_config.json\n",
            "Special tokens file saved in roberta-large-finetuned-code-mixed-DS/special_tokens_map.json\n",
            "/home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 497\n",
            "  Batch size = 32\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.017, 'learning_rate': 4.016064257028112e-08, 'epoch': 19.92}\n"
          ]
        },
        {
          "data": {
            "application/json": {
              "ascii": false,
              "bar_format": null,
              "colour": null,
              "elapsed": 0.015111684799194336,
              "initial": 0,
              "n": 0,
              "ncols": null,
              "nrows": null,
              "postfix": null,
              "prefix": "",
              "rate": null,
              "total": 16,
              "unit": "it",
              "unit_divisor": 1000,
              "unit_scale": false
            },
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "82f5080c89cd47f481b18f69558f752b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/16 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to roberta-large-finetuned-code-mixed-DS/checkpoint-4960\n",
            "Configuration saved in roberta-large-finetuned-code-mixed-DS/checkpoint-4960/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 3.134014844894409, 'eval_accuracy': 0.7203219315895373, 'eval_precision': 0.6584497599203482, 'eval_recall': 0.6548496560150875, 'eval_f1': 0.6558495378489484, 'eval_runtime': 5.3231, 'eval_samples_per_second': 93.367, 'eval_steps_per_second': 3.006, 'epoch': 19.92}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in roberta-large-finetuned-code-mixed-DS/checkpoint-4960/pytorch_model.bin\n",
            "tokenizer config file saved in roberta-large-finetuned-code-mixed-DS/checkpoint-4960/tokenizer_config.json\n",
            "Special tokens file saved in roberta-large-finetuned-code-mixed-DS/checkpoint-4960/special_tokens_map.json\n",
            "/home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from roberta-large-finetuned-code-mixed-DS/checkpoint-3472 (score: 0.6870287706994859).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'train_runtime': 5516.9346, 'train_samples_per_second': 14.414, 'train_steps_per_second': 0.903, 'train_loss': 0.1989313795698814, 'epoch': 20.0}\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9d1c0f756e4c4a28abbf231e1af4613b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁▄▄▄▇▅▆▄▄▄▇▆▇█▆▄▅▅▅▅</td></tr><tr><td>eval/f1</td><td>▁▇▅▃▆▄▆▃▄▃▇▆▇█▅▃▄▄▃▄</td></tr><tr><td>eval/loss</td><td>▁▁▁▂▂▃▄▅▆▆▆▇▇▇██████</td></tr><tr><td>eval/precision</td><td>▁▅▃▂▆▃▅▂▃▂▇▄▆█▅▂▄▃▃▃</td></tr><tr><td>eval/recall</td><td>▃█▇▄▄▃▆▂▂▂▆▅▅▆▃▁▂▂▁▂</td></tr><tr><td>eval/runtime</td><td>▁▁▁▆▄▃▂███▄▂▃▄▃▂▃▃▂▄</td></tr><tr><td>eval/samples_per_second</td><td>███▃▅▆▇▁▁▁▅▇▆▅▆▇▆▆▇▅</td></tr><tr><td>eval/steps_per_second</td><td>███▃▅▆▇▁▁▁▅▇▆▅▆▇▆▆▇▅</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>train/learning_rate</td><td>██▇▇▇▆▆▅▅▅▄▄▄▃▃▂▂▂▁▁</td></tr><tr><td>train/loss</td><td>█▆▅▄▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.72032</td></tr><tr><td>eval/f1</td><td>0.65585</td></tr><tr><td>eval/loss</td><td>3.13401</td></tr><tr><td>eval/precision</td><td>0.65845</td></tr><tr><td>eval/recall</td><td>0.65485</td></tr><tr><td>eval/runtime</td><td>5.3231</td></tr><tr><td>eval/samples_per_second</td><td>93.367</td></tr><tr><td>eval/steps_per_second</td><td>3.006</td></tr><tr><td>train/epoch</td><td>20.0</td></tr><tr><td>train/global_step</td><td>4980</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.017</td></tr><tr><td>train/total_flos</td><td>7.38179502465888e+16</td></tr><tr><td>train/train_loss</td><td>0.19893</td></tr><tr><td>train/train_runtime</td><td>5516.9346</td></tr><tr><td>train/train_samples_per_second</td><td>14.414</td></tr><tr><td>train/train_steps_per_second</td><td>0.903</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">roberta-large-code-mixed-DS</strong>: <a href=\"https://wandb.ai/diptesh/aggression_detection/runs/tcobsn8r\" target=\"_blank\">https://wandb.ai/diptesh/aggression_detection/runs/tcobsn8r</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20221001_231608-tcobsn8r/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "trainer = CustomTrainer(model_init=model_init,\n",
        "                        args=training_args,\n",
        "                        compute_metrics = compute_metrics,\n",
        "                        train_dataset = train_dataset,\n",
        "                        eval_dataset = valid_dataset,\n",
        "                        tokenizer = tokenizer, \n",
        "                        # callbacks = [EarlyStoppingCallback(early_stopping_patience = 2, early_stopping_threshold=0.0001)]\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# post-training analysis, testing, other logged code\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "gguBpeh4xGdy"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to roberta-large-finetuned-code-mixed-DS\n",
            "Configuration saved in roberta-large-finetuned-code-mixed-DS/config.json\n",
            "Model weights saved in roberta-large-finetuned-code-mixed-DS/pytorch_model.bin\n",
            "tokenizer config file saved in roberta-large-finetuned-code-mixed-DS/tokenizer_config.json\n",
            "Special tokens file saved in roberta-large-finetuned-code-mixed-DS/special_tokens_map.json\n"
          ]
        },
        {
          "data": {
            "application/json": {
              "ascii": false,
              "bar_format": null,
              "colour": null,
              "elapsed": 0.04187607765197754,
              "initial": 32768,
              "n": 32768,
              "ncols": null,
              "nrows": null,
              "postfix": null,
              "prefix": "Upload file pytorch_model.bin",
              "rate": null,
              "total": 1421588461,
              "unit": "B",
              "unit_divisor": 1024,
              "unit_scale": true
            },
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4cba808dcc5141c29a08a02b5c402417",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Upload file pytorch_model.bin:   0%|          | 32.0k/1.32G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "remote: Scanning LFS files for validity, may be slow...        \n",
            "remote: LFS file scan complete.        \n",
            "To https://huggingface.co/dipteshkanojia/roberta-large-finetuned-code-mixed-DS\n",
            "   d0a7517..fbf5a94  main -> main\n",
            "\n",
            "Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Text Classification', 'type': 'text-classification'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.7203219315895373}, {'name': 'Precision', 'type': 'precision', 'value': 0.6584497599203482}, {'name': 'Recall', 'type': 'recall', 'value': 0.6548496560150875}, {'name': 'F1', 'type': 'f1', 'value': 0.6558495378489484}]}\n",
            "To https://huggingface.co/dipteshkanojia/roberta-large-finetuned-code-mixed-DS\n",
            "   fbf5a94..10cb3f4  main -> main\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'https://huggingface.co/dipteshkanojia/roberta-large-finetuned-code-mixed-DS/commit/fbf5a944653897cf10393eae4e23c0bf36929319'"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.push_to_hub()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8DGegrFFB9c"
      },
      "source": [
        "## 8) Predictions and Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "uwWgMmrenkxF"
      },
      "outputs": [],
      "source": [
        "test_texts = list(test_df['Sentence'])\n",
        "test_labels = list(test_df['Label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "J18PaJADvljI"
      },
      "outputs": [],
      "source": [
        "test_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=510)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "aFnak-ItvrG-"
      },
      "outputs": [],
      "source": [
        "test_dataset = AggressionDataset(test_encodings, test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "qFi6MZz-g9xu"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Prediction *****\n",
            "  Num examples = 498\n",
            "  Batch size = 32\n",
            "/home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
          ]
        },
        {
          "data": {
            "application/json": {
              "ascii": false,
              "bar_format": null,
              "colour": null,
              "elapsed": 0.02583765983581543,
              "initial": 0,
              "n": 0,
              "ncols": null,
              "nrows": null,
              "postfix": null,
              "prefix": "",
              "rate": null,
              "total": 16,
              "unit": "it",
              "unit_divisor": 1000,
              "unit_scale": false
            },
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3e7a38f26c714e0680bfbbf3e8aab4fd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/16 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "preds_output_test = trainer.predict(test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "nQJfQMYWhAtz"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'test_loss': 2.5998706817626953,\n",
              " 'test_accuracy': 0.7670682730923695,\n",
              " 'test_precision': 0.7155506237784719,\n",
              " 'test_recall': 0.6951545464168518,\n",
              " 'test_f1': 0.7034700734726617,\n",
              " 'test_runtime': 5.0903,\n",
              " 'test_samples_per_second': 97.834,\n",
              " 'test_steps_per_second': 3.143}"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "preds_output_test.metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "tR_TsEhihCtm"
      },
      "outputs": [],
      "source": [
        "y_preds_test = np.argmax(preds_output_test.predictions, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "PfZhPHNFhE3B"
      },
      "outputs": [],
      "source": [
        "y_valid_test = np.array(test_dataset.labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "dZRj0AV4hGcP"
      },
      "outputs": [],
      "source": [
        "map_dt = {0:'NAG', 1:'CAG', 2:'OAG'}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "sgugEinyhH_X"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         NAG       0.86      0.92      0.89       268\n",
            "         CAG       0.64      0.62      0.63       136\n",
            "         OAG       0.65      0.54      0.59        94\n",
            "\n",
            "    accuracy                           0.77       498\n",
            "   macro avg       0.72      0.70      0.70       498\n",
            "weighted avg       0.76      0.77      0.76       498\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(classification_report(y_valid_test, y_preds_test, target_names=list(map_dt.values())))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "KCcc6g3NhLj7"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "y_valid_trying = map(lambda x : map_dt[x], y_valid_test)\n",
        "y_valid_trying = list(y_valid_trying)\n",
        "\n",
        "y_preds_trying = map(lambda x : map_dt[x], y_preds_test)\n",
        "y_preds_trying = list(y_preds_trying)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "IwHUVo5PBE-x"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f0257783400>"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "cm_labels = np.unique(y_valid_trying)\n",
        "cm_array = confusion_matrix(y_valid_trying, y_preds_trying)\n",
        "cm_array_df = pd.DataFrame(cm_array, index=cm_labels, columns=cm_labels)\n",
        "sns.heatmap(cm_array_df, annot=True, annot_kws={\"size\": 12}) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.8.13 ('aggDet')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "0c1cc14d24d579ea9f448eff41282ce5bee110707ee119424f8b85e664f18efb"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
