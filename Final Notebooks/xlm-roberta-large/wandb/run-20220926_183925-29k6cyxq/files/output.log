/home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** Running Evaluation *****
  Num examples = 1224
  Batch size = 8
{'loss': 1.1358, 'learning_rate': 3.94721890998806e-05, 'epoch': 0.25}
{'eval_loss': 1.1002637147903442, 'eval_accuracy': 0.44362745098039214, 'eval_precision': 0.14787581699346405, 'eval_recall': 0.3333333333333333, 'eval_f1': 0.20486700622524054, 'eval_runtime': 20.9086, 'eval_samples_per_second': 58.54, 'eval_steps_per_second': 7.318, 'epoch': 0.25}
/home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Saving model checkpoint to xlm-roberta-large-finetuned-TRAC-DS-new/checkpoint-612
Configuration saved in xlm-roberta-large-finetuned-TRAC-DS-new/checkpoint-612/config.json
Model weights saved in xlm-roberta-large-finetuned-TRAC-DS-new/checkpoint-612/pytorch_model.bin
tokenizer config file saved in xlm-roberta-large-finetuned-TRAC-DS-new/checkpoint-612/tokenizer_config.json
Special tokens file saved in xlm-roberta-large-finetuned-TRAC-DS-new/checkpoint-612/special_tokens_map.json
tokenizer config file saved in xlm-roberta-large-finetuned-TRAC-DS-new/tokenizer_config.json
Special tokens file saved in xlm-roberta-large-finetuned-TRAC-DS-new/special_tokens_map.json
/home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** Running Evaluation *****
  Num examples = 1224
  Batch size = 8
{'loss': 1.1199, 'learning_rate': 3.775673818885042e-05, 'epoch': 0.5}
/home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Saving model checkpoint to xlm-roberta-large-finetuned-TRAC-DS-new/checkpoint-1224
Configuration saved in xlm-roberta-large-finetuned-TRAC-DS-new/checkpoint-1224/config.json
{'eval_loss': 1.1129651069641113, 'eval_accuracy': 0.44362745098039214, 'eval_precision': 0.14787581699346405, 'eval_recall': 0.3333333333333333, 'eval_f1': 0.20486700622524054, 'eval_runtime': 20.6887, 'eval_samples_per_second': 59.163, 'eval_steps_per_second': 7.395, 'epoch': 0.5}
Model weights saved in xlm-roberta-large-finetuned-TRAC-DS-new/checkpoint-1224/pytorch_model.bin
tokenizer config file saved in xlm-roberta-large-finetuned-TRAC-DS-new/checkpoint-1224/tokenizer_config.json
Special tokens file saved in xlm-roberta-large-finetuned-TRAC-DS-new/checkpoint-1224/special_tokens_map.json
tokenizer config file saved in xlm-roberta-large-finetuned-TRAC-DS-new/tokenizer_config.json
Special tokens file saved in xlm-roberta-large-finetuned-TRAC-DS-new/special_tokens_map.json
/home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
{'loss': 1.1221, 'learning_rate': 3.604128727782025e-05, 'epoch': 0.75}
***** Running Evaluation *****
  Num examples = 1224
  Batch size = 8
/home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Saving model checkpoint to xlm-roberta-large-finetuned-TRAC-DS-new/checkpoint-1836
Configuration saved in xlm-roberta-large-finetuned-TRAC-DS-new/checkpoint-1836/config.json
{'eval_loss': 1.0992172956466675, 'eval_accuracy': 0.3341503267973856, 'eval_precision': 0.1113834422657952, 'eval_recall': 0.3333333333333333, 'eval_f1': 0.16697285160236783, 'eval_runtime': 20.7472, 'eval_samples_per_second': 58.996, 'eval_steps_per_second': 7.374, 'epoch': 0.75}
Model weights saved in xlm-roberta-large-finetuned-TRAC-DS-new/checkpoint-1836/pytorch_model.bin
tokenizer config file saved in xlm-roberta-large-finetuned-TRAC-DS-new/checkpoint-1836/tokenizer_config.json
Special tokens file saved in xlm-roberta-large-finetuned-TRAC-DS-new/checkpoint-1836/special_tokens_map.json
/home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
{'loss': 1.1185, 'learning_rate': 3.4325836366790075e-05, 'epoch': 1.0}
***** Running Evaluation *****
  Num examples = 1224
  Batch size = 8
/home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
{'eval_loss': 1.1097263097763062, 'eval_accuracy': 0.3341503267973856, 'eval_precision': 0.1113834422657952, 'eval_recall': 0.3333333333333333, 'eval_f1': 0.16697285160236783, 'eval_runtime': 20.6324, 'eval_samples_per_second': 59.324, 'eval_steps_per_second': 7.416, 'epoch': 1.0}
Saving model checkpoint to xlm-roberta-large-finetuned-TRAC-DS-new/checkpoint-2448
Configuration saved in xlm-roberta-large-finetuned-TRAC-DS-new/checkpoint-2448/config.json
Model weights saved in xlm-roberta-large-finetuned-TRAC-DS-new/checkpoint-2448/pytorch_model.bin
tokenizer config file saved in xlm-roberta-large-finetuned-TRAC-DS-new/checkpoint-2448/tokenizer_config.json
Special tokens file saved in xlm-roberta-large-finetuned-TRAC-DS-new/checkpoint-2448/special_tokens_map.json
tokenizer config file saved in xlm-roberta-large-finetuned-TRAC-DS-new/tokenizer_config.json
Special tokens file saved in xlm-roberta-large-finetuned-TRAC-DS-new/special_tokens_map.json
/home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
{'loss': 1.1152, 'learning_rate': 3.26103854557599e-05, 'epoch': 1.25}
***** Running Evaluation *****
  Num examples = 1224
  Batch size = 8
{'eval_loss': 1.1750001907348633, 'eval_accuracy': 0.44362745098039214, 'eval_precision': 0.14787581699346405, 'eval_recall': 0.3333333333333333, 'eval_f1': 0.20486700622524054, 'eval_runtime': 20.6219, 'eval_samples_per_second': 59.354, 'eval_steps_per_second': 7.419, 'epoch': 1.25}
/home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Saving model checkpoint to xlm-roberta-large-finetuned-TRAC-DS-new/checkpoint-3060
Configuration saved in xlm-roberta-large-finetuned-TRAC-DS-new/checkpoint-3060/config.json
Model weights saved in xlm-roberta-large-finetuned-TRAC-DS-new/checkpoint-3060/pytorch_model.bin
tokenizer config file saved in xlm-roberta-large-finetuned-TRAC-DS-new/checkpoint-3060/tokenizer_config.json
Special tokens file saved in xlm-roberta-large-finetuned-TRAC-DS-new/checkpoint-3060/special_tokens_map.json
/home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
{'loss': 1.11, 'learning_rate': 3.089493454472972e-05, 'epoch': 1.5}
***** Running Evaluation *****
  Num examples = 1224
  Batch size = 8
{'eval_loss': 1.1854444742202759, 'eval_accuracy': 0.44362745098039214, 'eval_precision': 0.14787581699346405, 'eval_recall': 0.3333333333333333, 'eval_f1': 0.20486700622524054, 'eval_runtime': 20.6051, 'eval_samples_per_second': 59.403, 'eval_steps_per_second': 7.425, 'epoch': 1.5}
/home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Saving model checkpoint to xlm-roberta-large-finetuned-TRAC-DS-new/checkpoint-3672
Configuration saved in xlm-roberta-large-finetuned-TRAC-DS-new/checkpoint-3672/config.json
Model weights saved in xlm-roberta-large-finetuned-TRAC-DS-new/checkpoint-3672/pytorch_model.bin
tokenizer config file saved in xlm-roberta-large-finetuned-TRAC-DS-new/checkpoint-3672/tokenizer_config.json
Special tokens file saved in xlm-roberta-large-finetuned-TRAC-DS-new/checkpoint-3672/special_tokens_map.json
tokenizer config file saved in xlm-roberta-large-finetuned-TRAC-DS-new/tokenizer_config.json
Special tokens file saved in xlm-roberta-large-finetuned-TRAC-DS-new/special_tokens_map.json
/home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
{'loss': 1.1095, 'learning_rate': 2.917948363369955e-05, 'epoch': 1.75}
***** Running Evaluation *****
  Num examples = 1224
  Batch size = 8
{'eval_loss': 1.221399188041687, 'eval_accuracy': 0.44362745098039214, 'eval_precision': 0.14787581699346405, 'eval_recall': 0.3333333333333333, 'eval_f1': 0.20486700622524054, 'eval_runtime': 20.9102, 'eval_samples_per_second': 58.536, 'eval_steps_per_second': 7.317, 'epoch': 1.75}
/home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Saving model checkpoint to xlm-roberta-large-finetuned-TRAC-DS-new/checkpoint-4284
Configuration saved in xlm-roberta-large-finetuned-TRAC-DS-new/checkpoint-4284/config.json
Model weights saved in xlm-roberta-large-finetuned-TRAC-DS-new/checkpoint-4284/pytorch_model.bin
tokenizer config file saved in xlm-roberta-large-finetuned-TRAC-DS-new/checkpoint-4284/tokenizer_config.json
Special tokens file saved in xlm-roberta-large-finetuned-TRAC-DS-new/checkpoint-4284/special_tokens_map.json
/home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
{'loss': 1.1029, 'learning_rate': 2.7464032722669372e-05, 'epoch': 2.0}
***** Running Evaluation *****
  Num examples = 1224
  Batch size = 8
{'eval_loss': 1.1920428276062012, 'eval_accuracy': 0.44362745098039214, 'eval_precision': 0.14787581699346405, 'eval_recall': 0.3333333333333333, 'eval_f1': 0.20486700622524054, 'eval_runtime': 20.7221, 'eval_samples_per_second': 59.067, 'eval_steps_per_second': 7.383, 'epoch': 2.0}
/home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Saving model checkpoint to xlm-roberta-large-finetuned-TRAC-DS-new/checkpoint-4896
Configuration saved in xlm-roberta-large-finetuned-TRAC-DS-new/checkpoint-4896/config.json
Model weights saved in xlm-roberta-large-finetuned-TRAC-DS-new/checkpoint-4896/pytorch_model.bin
tokenizer config file saved in xlm-roberta-large-finetuned-TRAC-DS-new/checkpoint-4896/tokenizer_config.json
Special tokens file saved in xlm-roberta-large-finetuned-TRAC-DS-new/checkpoint-4896/special_tokens_map.json
tokenizer config file saved in xlm-roberta-large-finetuned-TRAC-DS-new/tokenizer_config.json
Special tokens file saved in xlm-roberta-large-finetuned-TRAC-DS-new/special_tokens_map.json
/home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** Running Evaluation *****
  Num examples = 1224
  Batch size = 8
{'loss': 1.11, 'learning_rate': 2.5748581811639195e-05, 'epoch': 2.25}
/home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Saving model checkpoint to xlm-roberta-large-finetuned-TRAC-DS-new/checkpoint-5508
Configuration saved in xlm-roberta-large-finetuned-TRAC-DS-new/checkpoint-5508/config.json
{'eval_loss': 1.1084833145141602, 'eval_accuracy': 0.3341503267973856, 'eval_precision': 0.1113834422657952, 'eval_recall': 0.3333333333333333, 'eval_f1': 0.16697285160236783, 'eval_runtime': 20.6571, 'eval_samples_per_second': 59.253, 'eval_steps_per_second': 7.407, 'epoch': 2.25}
Model weights saved in xlm-roberta-large-finetuned-TRAC-DS-new/checkpoint-5508/pytorch_model.bin
tokenizer config file saved in xlm-roberta-large-finetuned-TRAC-DS-new/checkpoint-5508/tokenizer_config.json
Special tokens file saved in xlm-roberta-large-finetuned-TRAC-DS-new/checkpoint-5508/special_tokens_map.json
/home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** Running Evaluation *****
  Num examples = 1224
  Batch size = 8
{'loss': 1.109, 'learning_rate': 2.4033130900609025e-05, 'epoch': 2.5}
/home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Saving model checkpoint to xlm-roberta-large-finetuned-TRAC-DS-new/checkpoint-6120
Configuration saved in xlm-roberta-large-finetuned-TRAC-DS-new/checkpoint-6120/config.json
{'eval_loss': 1.1713088750839233, 'eval_accuracy': 0.44362745098039214, 'eval_precision': 0.14787581699346405, 'eval_recall': 0.3333333333333333, 'eval_f1': 0.20486700622524054, 'eval_runtime': 21.3592, 'eval_samples_per_second': 57.306, 'eval_steps_per_second': 7.163, 'epoch': 2.5}
Model weights saved in xlm-roberta-large-finetuned-TRAC-DS-new/checkpoint-6120/pytorch_model.bin
tokenizer config file saved in xlm-roberta-large-finetuned-TRAC-DS-new/checkpoint-6120/tokenizer_config.json
Special tokens file saved in xlm-roberta-large-finetuned-TRAC-DS-new/checkpoint-6120/special_tokens_map.json
tokenizer config file saved in xlm-roberta-large-finetuned-TRAC-DS-new/tokenizer_config.json
Special tokens file saved in xlm-roberta-large-finetuned-TRAC-DS-new/special_tokens_map.json
/home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** Running Evaluation *****
  Num examples = 1224
  Batch size = 8
{'loss': 1.1082, 'learning_rate': 2.231767998957885e-05, 'epoch': 2.75}
/home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Saving model checkpoint to xlm-roberta-large-finetuned-TRAC-DS-new/checkpoint-6732
Configuration saved in xlm-roberta-large-finetuned-TRAC-DS-new/checkpoint-6732/config.json
{'eval_loss': 1.1809382438659668, 'eval_accuracy': 0.44362745098039214, 'eval_precision': 0.14787581699346405, 'eval_recall': 0.3333333333333333, 'eval_f1': 0.20486700622524054, 'eval_runtime': 20.4895, 'eval_samples_per_second': 59.738, 'eval_steps_per_second': 7.467, 'epoch': 2.75}
Model weights saved in xlm-roberta-large-finetuned-TRAC-DS-new/checkpoint-6732/pytorch_model.bin
tokenizer config file saved in xlm-roberta-large-finetuned-TRAC-DS-new/checkpoint-6732/tokenizer_config.json
Special tokens file saved in xlm-roberta-large-finetuned-TRAC-DS-new/checkpoint-6732/special_tokens_map.json
/home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** Running Evaluation *****
  Num examples = 1224
  Batch size = 8
{'loss': 1.1051, 'learning_rate': 2.0602229078548672e-05, 'epoch': 3.0}
/home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Saving model checkpoint to xlm-roberta-large-finetuned-TRAC-DS-new/checkpoint-7344
Configuration saved in xlm-roberta-large-finetuned-TRAC-DS-new/checkpoint-7344/config.json
{'eval_loss': 1.2298939228057861, 'eval_accuracy': 0.44362745098039214, 'eval_precision': 0.14787581699346405, 'eval_recall': 0.3333333333333333, 'eval_f1': 0.20486700622524054, 'eval_runtime': 20.6388, 'eval_samples_per_second': 59.306, 'eval_steps_per_second': 7.413, 'epoch': 3.0}
Model weights saved in xlm-roberta-large-finetuned-TRAC-DS-new/checkpoint-7344/pytorch_model.bin
tokenizer config file saved in xlm-roberta-large-finetuned-TRAC-DS-new/checkpoint-7344/tokenizer_config.json
Special tokens file saved in xlm-roberta-large-finetuned-TRAC-DS-new/checkpoint-7344/special_tokens_map.json
tokenizer config file saved in xlm-roberta-large-finetuned-TRAC-DS-new/tokenizer_config.json
Special tokens file saved in xlm-roberta-large-finetuned-TRAC-DS-new/special_tokens_map.json
/home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** Running Evaluation *****
  Num examples = 1224
  Batch size = 8
{'loss': 1.0973, 'learning_rate': 1.8886778167518495e-05, 'epoch': 3.25}
/home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Saving model checkpoint to xlm-roberta-large-finetuned-TRAC-DS-new/checkpoint-7956
Configuration saved in xlm-roberta-large-finetuned-TRAC-DS-new/checkpoint-7956/config.json
{'eval_loss': 1.1869558095932007, 'eval_accuracy': 0.44362745098039214, 'eval_precision': 0.14787581699346405, 'eval_recall': 0.3333333333333333, 'eval_f1': 0.20486700622524054, 'eval_runtime': 20.9877, 'eval_samples_per_second': 58.32, 'eval_steps_per_second': 7.29, 'epoch': 3.25}
Model weights saved in xlm-roberta-large-finetuned-TRAC-DS-new/checkpoint-7956/pytorch_model.bin
tokenizer config file saved in xlm-roberta-large-finetuned-TRAC-DS-new/checkpoint-7956/tokenizer_config.json
Special tokens file saved in xlm-roberta-large-finetuned-TRAC-DS-new/checkpoint-7956/special_tokens_map.json
/home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
{'loss': 1.1016, 'learning_rate': 1.7171327256488322e-05, 'epoch': 3.5}
***** Running Evaluation *****
  Num examples = 1224
  Batch size = 8
/home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Saving model checkpoint to xlm-roberta-large-finetuned-TRAC-DS-new/checkpoint-8568
Configuration saved in xlm-roberta-large-finetuned-TRAC-DS-new/checkpoint-8568/config.json
{'eval_loss': 1.1890426874160767, 'eval_accuracy': 0.44362745098039214, 'eval_precision': 0.14787581699346405, 'eval_recall': 0.3333333333333333, 'eval_f1': 0.20486700622524054, 'eval_runtime': 20.6563, 'eval_samples_per_second': 59.256, 'eval_steps_per_second': 7.407, 'epoch': 3.5}
Model weights saved in xlm-roberta-large-finetuned-TRAC-DS-new/checkpoint-8568/pytorch_model.bin
tokenizer config file saved in xlm-roberta-large-finetuned-TRAC-DS-new/checkpoint-8568/tokenizer_config.json
Special tokens file saved in xlm-roberta-large-finetuned-TRAC-DS-new/checkpoint-8568/special_tokens_map.json
tokenizer config file saved in xlm-roberta-large-finetuned-TRAC-DS-new/tokenizer_config.json
Special tokens file saved in xlm-roberta-large-finetuned-TRAC-DS-new/special_tokens_map.json
/home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
{'loss': 1.094, 'learning_rate': 1.545587634545815e-05, 'epoch': 3.75}
***** Running Evaluation *****
  Num examples = 1224
  Batch size = 8
/home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Saving model checkpoint to xlm-roberta-large-finetuned-TRAC-DS-new/checkpoint-9180
Configuration saved in xlm-roberta-large-finetuned-TRAC-DS-new/checkpoint-9180/config.json
{'eval_loss': 1.1652626991271973, 'eval_accuracy': 0.44362745098039214, 'eval_precision': 0.14787581699346405, 'eval_recall': 0.3333333333333333, 'eval_f1': 0.20486700622524054, 'eval_runtime': 20.9202, 'eval_samples_per_second': 58.508, 'eval_steps_per_second': 7.314, 'epoch': 3.75}
Model weights saved in xlm-roberta-large-finetuned-TRAC-DS-new/checkpoint-9180/pytorch_model.bin
tokenizer config file saved in xlm-roberta-large-finetuned-TRAC-DS-new/checkpoint-9180/tokenizer_config.json
Special tokens file saved in xlm-roberta-large-finetuned-TRAC-DS-new/checkpoint-9180/special_tokens_map.json
/home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** Running Evaluation *****
  Num examples = 1224
  Batch size = 8
{'loss': 1.1035, 'learning_rate': 1.3740425434427972e-05, 'epoch': 4.0}
using `logging_steps` to initialize `eval_steps` to 612
PyTorch: setting up devices
loading configuration file https://huggingface.co/xlm-roberta-large/resolve/main/config.json from cache at /home/diptesh/.cache/huggingface/transformers/4d7a1550c9ab8701667bc307a1213c040fcc06dc87a5e4994e72aecc0d7e0337.842c7737719967568f4691849854475018d6cf7ce21f52576bb6e0d10091bd3c
Model config XLMRobertaConfig {
  "_name_or_path": "xlm-roberta-large",
  "architectures": [
    "XLMRobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "xlm-roberta",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "output_past": true,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.20.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 250002
}
loading weights file https://huggingface.co/xlm-roberta-large/resolve/main/pytorch_model.bin from cache at /home/diptesh/.cache/huggingface/transformers/4b3ca85a63804fb7cd317765d9de19ce6208ee0fc9691b209384ee7cfd9cb3b9.64b4693d874c772310b8acda9a1193cfade77d56795a9b488e612f198b68f6f7
Some weights of the model checkpoint at xlm-roberta-large were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.dense.bias']
- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-large and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/diptesh/workspace/AggressionDetection-IIITL/Final Notebooks/xlm-roberta-large/xlm-roberta-large-finetuned-TRAC-DS-new is already a clone of https://huggingface.co/dipteshkanojia/xlm-roberta-large-finetuned-TRAC-DS-new. Make sure you pull the latest changes with `repo.git_pull()`.
loading configuration file https://huggingface.co/xlm-roberta-large/resolve/main/config.json from cache at /home/diptesh/.cache/huggingface/transformers/4d7a1550c9ab8701667bc307a1213c040fcc06dc87a5e4994e72aecc0d7e0337.842c7737719967568f4691849854475018d6cf7ce21f52576bb6e0d10091bd3c
Model config XLMRobertaConfig {
  "_name_or_path": "xlm-roberta-large",
  "architectures": [
    "XLMRobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "xlm-roberta",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "output_past": true,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.20.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 250002
}
loading weights file https://huggingface.co/xlm-roberta-large/resolve/main/pytorch_model.bin from cache at /home/diptesh/.cache/huggingface/transformers/4b3ca85a63804fb7cd317765d9de19ce6208ee0fc9691b209384ee7cfd9cb3b9.64b4693d874c772310b8acda9a1193cfade77d56795a9b488e612f198b68f6f7
Some weights of the model checkpoint at xlm-roberta-large were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.dense.bias']
- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-large and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
***** Running training *****
  Num examples = 9795
  Num Epochs = 6
  Instantaneous batch size per device = 2
  Total train batch size (w. parallel, distributed & accumulation) = 4
  Gradient Accumulation steps = 1
  Total optimization steps = 14694
Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
/home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** Running Evaluation *****
  Num examples = 1224
  Batch size = 8
{'loss': 1.0895, 'learning_rate': 9.583503470804411e-06, 'epoch': 0.25}
using `logging_steps` to initialize `eval_steps` to 612
PyTorch: setting up devices
using `logging_steps` to initialize `eval_steps` to 612
PyTorch: setting up devices
loading configuration file https://huggingface.co/xlm-roberta-large/resolve/main/config.json from cache at /home/diptesh/.cache/huggingface/transformers/4d7a1550c9ab8701667bc307a1213c040fcc06dc87a5e4994e72aecc0d7e0337.842c7737719967568f4691849854475018d6cf7ce21f52576bb6e0d10091bd3c
Model config XLMRobertaConfig {
  "_name_or_path": "xlm-roberta-large",
  "architectures": [
    "XLMRobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "xlm-roberta",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "output_past": true,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.20.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 250002
}
loading weights file https://huggingface.co/xlm-roberta-large/resolve/main/pytorch_model.bin from cache at /home/diptesh/.cache/huggingface/transformers/4b3ca85a63804fb7cd317765d9de19ce6208ee0fc9691b209384ee7cfd9cb3b9.64b4693d874c772310b8acda9a1193cfade77d56795a9b488e612f198b68f6f7
Some weights of the model checkpoint at xlm-roberta-large were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.dense.bias']
- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-large and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/diptesh/workspace/AggressionDetection-IIITL/Final Notebooks/xlm-roberta-large/xlm-roberta-large-finetuned-TRAC-DS-new is already a clone of https://huggingface.co/dipteshkanojia/xlm-roberta-large-finetuned-TRAC-DS-new. Make sure you pull the latest changes with `repo.git_pull()`.
loading configuration file https://huggingface.co/xlm-roberta-large/resolve/main/config.json from cache at /home/diptesh/.cache/huggingface/transformers/4d7a1550c9ab8701667bc307a1213c040fcc06dc87a5e4994e72aecc0d7e0337.842c7737719967568f4691849854475018d6cf7ce21f52576bb6e0d10091bd3c
Model config XLMRobertaConfig {
  "_name_or_path": "xlm-roberta-large",
  "architectures": [
    "XLMRobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "xlm-roberta",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "output_past": true,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.20.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 250002
}
loading weights file https://huggingface.co/xlm-roberta-large/resolve/main/pytorch_model.bin from cache at /home/diptesh/.cache/huggingface/transformers/4b3ca85a63804fb7cd317765d9de19ce6208ee0fc9691b209384ee7cfd9cb3b9.64b4693d874c772310b8acda9a1193cfade77d56795a9b488e612f198b68f6f7
Some weights of the model checkpoint at xlm-roberta-large were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.dense.bias']
- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-large and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
***** Running training *****
  Num examples = 9795
  Num Epochs = 6
  Instantaneous batch size per device = 2
  Total train batch size (w. parallel, distributed & accumulation) = 4
  Gradient Accumulation steps = 1
  Total optimization steps = 14694
