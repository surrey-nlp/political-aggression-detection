{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LF0eI-2QE_ky"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHu-iZKrS6Tu"
      },
      "source": [
        "## 1) Installing libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "cFQX2EGiXgos"
      },
      "outputs": [],
      "source": [
        "# !pip install transformers\n",
        "# !pip install datasets\n",
        "# !pip install wandb\n",
        "import os\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "le8H055mTeqs"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from datasets import load_dataset, Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0LlF1SVVvNM"
      },
      "source": [
        "## 2) Loading dataset (from HF)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "QPG3xAkTYsw7"
      },
      "outputs": [],
      "source": [
        "# enter your personal read token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "xdJCpTLOXiKv"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f71e44f5e34747759fd82366af6274fe",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "lTW-jsAmQesI"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using custom data configuration IIIT-L--total_code_mixed-c86de67ddd2696fa\n",
            "Reusing dataset csv (/home/diptesh/.cache/huggingface/datasets/IIIT-L___csv/IIIT-L--total_code_mixed-c86de67ddd2696fa/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58)\n"
          ]
        },
        {
          "data": {
            "application/json": {
              "ascii": false,
              "bar_format": null,
              "colour": null,
              "elapsed": 0.03486943244934082,
              "initial": 0,
              "n": 0,
              "ncols": null,
              "nrows": null,
              "postfix": null,
              "prefix": "",
              "rate": null,
              "total": 3,
              "unit": "it",
              "unit_divisor": 1000,
              "unit_scale": false
            },
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "648372ee7ef34a6083deea98efbfac2a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['Sentence', 'Label'],\n",
            "        num_rows: 3976\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['Sentence', 'Label'],\n",
            "        num_rows: 498\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['Sentence', 'Label'],\n",
            "        num_rows: 497\n",
            "    })\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "aggression_dataset = load_dataset(\"IIIT-L/total_code_mixed\", use_auth_token=True)\n",
        "\n",
        "print(aggression_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "43SsJM-aTlg7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['Sentence', 'Label'],\n",
              "    num_rows: 3976\n",
              "})"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_ds = aggression_dataset['train']\n",
        "train_ds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7T67guUEX0Nw"
      },
      "source": [
        "## 3) Converting to dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ZxPRh0hUUQz6"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentence</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>We don't spend one hour and rush off from our ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Shivani my classmate</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>True reviewerÃƒÂ°Ã…Â¸Ã¢â€žÂ¢Ã‚Â</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Royal C***yas Bangalore ÃƒÂ°Ã…Â¸Ã‹Å“Ã¢â‚¬Å¡ÃƒÂ...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>*PURE DESH ME AFRA TAFRI KA MAHOOL HAI...INDIA...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            Sentence  Label\n",
              "0  We don't spend one hour and rush off from our ...      1\n",
              "1                               Shivani my classmate      0\n",
              "2                   True reviewerÃƒÂ°Ã…Â¸Ã¢â€žÂ¢Ã‚Â      0\n",
              "3  Royal C***yas Bangalore ÃƒÂ°Ã…Â¸Ã‹Å“Ã¢â‚¬Å¡ÃƒÂ...      2\n",
              "4  *PURE DESH ME AFRA TAFRI KA MAHOOL HAI...INDIA...      2"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "aggression_dataset.set_format(type='pandas')\n",
        "train_df = aggression_dataset['train'][:]\n",
        "valid_df = aggression_dataset['validation'][:]\n",
        "\n",
        "train_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "IJsiywb_ofVt"
      },
      "outputs": [],
      "source": [
        "test_df = aggression_dataset['test'][:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "5LhCKCB4sMCa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0    2137\n",
              "1    1086\n",
              "2     753\n",
              "Name: Label, dtype: int64"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df['Label'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "bqe00WZ_IP2i"
      },
      "outputs": [],
      "source": [
        "# 3976\n",
        "# NAG-CAG-OAG (0-1-2) = 0.54-0.27-0.19"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFKTp8WlXubz"
      },
      "source": [
        "Seeing Distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "9tnlCy6dLRgi"
      },
      "outputs": [],
      "source": [
        "disb_df = train_df.copy(deep=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "wOdtcgKiXLZD"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages/matplotlib/cbook/__init__.py:1376: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  X = np.atleast_1d(X.T if isinstance(X, np.ndarray) else np.asarray(X))\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "disb_df['Words per sentence'] = disb_df['Sentence'].str.split().apply(len)\n",
        "disb_df.boxplot('Words per sentence', by='Label', grid=False, showfliers=False, color='black')\n",
        "plt.suptitle(\"\")\n",
        "plt.xlabel(\"\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5sA4SbW4jmYd"
      },
      "source": [
        "## 4) Tokenizing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "60uCbqkGjo0-"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "CwDB9kRGkG5L"
      },
      "outputs": [],
      "source": [
        "model_ckpt = 'xlm-roberta-large'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-iNzn8oleHo",
        "outputId": "0215f187-91cc-4de9-88f4-af75ecab1cd1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "250002"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.vocab_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "mk_bg4Pat9jw"
      },
      "outputs": [],
      "source": [
        "train_texts = list(train_df['Sentence'])\n",
        "train_labels = list(train_df['Label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "btVmfHrblxqm"
      },
      "outputs": [],
      "source": [
        "valid_texts = list(valid_df['Sentence'])\n",
        "valid_labels = list(valid_df['Label'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8RWWM8sMhyF"
      },
      "source": [
        "## 5) Encoding train-valid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "YV9Fz--nt8X_"
      },
      "outputs": [],
      "source": [
        "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
        "valid_encodings = tokenizer(valid_texts, truncation=True, padding=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Mc6Mpnqbuwx1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "class AggressionDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "mGql29l6ag6N"
      },
      "outputs": [],
      "source": [
        "train_dataset = AggressionDataset(train_encodings, train_labels)\n",
        "valid_dataset = AggressionDataset(valid_encodings, valid_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENs2HmKAanBd"
      },
      "source": [
        "## 6) Setting classification model and evaluation metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "DFmLAL7RbtPe"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "1JfA9ODa83rR"
      },
      "outputs": [],
      "source": [
        "# Use in case of CUDA memory error\n",
        "\n",
        "# import gc\n",
        "\n",
        "# gc.collect()\n",
        "# torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SwnXMX_Hap0V",
        "outputId": "596089c0-7061-4d83-8c0f-573804f42ea9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "num_labels = 3\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "\n",
        "def model_init():\n",
        "    model = (AutoModelForSequenceClassification.from_pretrained(model_ckpt, num_labels=num_labels))\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "IR3ZFBIjcF3H"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, plot_confusion_matrix\n",
        "\n",
        "def compute_metrics(pred):\n",
        "  labels = pred.label_ids\n",
        "  preds = pred.predictions.argmax(-1)\n",
        "\n",
        "  f1 = f1_score(labels, preds, average='macro')\n",
        "  precision = precision_score(labels, preds, average='macro')\n",
        "  recall = recall_score(labels, preds, average='macro')\n",
        "  acc = accuracy_score(labels, preds)\n",
        "  return {'accuracy': acc, 'precision': precision, 'recall': recall, 'f1': f1}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_1OptUlxh9-"
      },
      "source": [
        "## 7) Fine-tuning, visualizing training, saving model to HF  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "KGSxhrQ0vsfs"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdiptesh\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import wandb\n",
        "\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YtVAykzCv7vZ",
        "outputId": "128d694c-7f5c-4e87-82f1-5518427c2ec4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "env: WANDB_PROJECT=aggression_detection\n"
          ]
        }
      ],
      "source": [
        "%env WANDB_PROJECT = aggression_detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "EDakmiAHc150"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer, TrainingArguments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "_LlQYDjndBFG"
      },
      "outputs": [],
      "source": [
        "# Defining hyperparameters\n",
        "eval_batch_size = 16\n",
        "logging_steps = len(train_texts) // eval_batch_size\n",
        "model_name = f\"{model_ckpt}-finetuned-code-mixed-DS\"\n",
        "training_args = TrainingArguments(output_dir=model_name,\n",
        "                                  num_train_epochs=4,\n",
        "                                  learning_rate=1e-05,\n",
        "                                  per_device_train_batch_size=4,\n",
        "                                  per_device_eval_batch_size=8,\n",
        "                                  weight_decay=0.001,\n",
        "                                  evaluation_strategy='steps',\n",
        "                                  save_strategy='steps',\n",
        "                                  max_steps=-1,\n",
        "                                  warmup_ratio=0.0,\n",
        "                                  seed=43,\n",
        "                                  data_seed=4,\n",
        "                                  metric_for_best_model=\"eval_f1\",\n",
        "                                  greater_is_better=True,\n",
        "                                  load_best_model_at_end=True, \n",
        "                                  disable_tqdm=False,\n",
        "                                  logging_steps=logging_steps,\n",
        "                                  save_steps=logging_steps,\n",
        "                                  log_level='info', \n",
        "                                  report_to=\"wandb\", \n",
        "                                  run_name=\"xlmr-large-code-mixed-DS\",\n",
        "                                  push_to_hub=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "bC3nDg818V3U"
      },
      "outputs": [],
      "source": [
        "# import gc\n",
        "\n",
        "# gc.collect()\n",
        "# torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "Moy_vPC1XsQ7"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "\n",
        "class CustomTrainer(Trainer):\n",
        "  def compute_loss(self, model, inputs, return_outputs=False):\n",
        "    # device = torch.device('cuda')\n",
        "    # inputs.to(device)\n",
        "    labels = inputs.get(\"labels\")\n",
        "    # forward pass\n",
        "    outputs = model(**inputs)\n",
        "    logits = outputs.get(\"logits\")\n",
        "    # compute custom loss (suppose one has 3 labels with different weights)\n",
        "    loss_fct = nn.CrossEntropyLoss(weight=torch.tensor([0.17, 0.27, 0.56]).to(device))\n",
        "    loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
        "    return (loss, outputs) if return_outputs else loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "a-a-65FPl5YL"
      },
      "outputs": [],
      "source": [
        "from transformers import EarlyStoppingCallback"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "KJDDBeXSoSf5"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7d3a66d97b02471fa74fcbf98422c711",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# enter your personal write token here\n",
        "\n",
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "bgj9CC3qeD22"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loading configuration file https://huggingface.co/xlm-roberta-large/resolve/main/config.json from cache at /home/diptesh/.cache/huggingface/transformers/4d7a1550c9ab8701667bc307a1213c040fcc06dc87a5e4994e72aecc0d7e0337.842c7737719967568f4691849854475018d6cf7ce21f52576bb6e0d10091bd3c\n",
            "Model config XLMRobertaConfig {\n",
            "  \"_name_or_path\": \"xlm-roberta-large\",\n",
            "  \"architectures\": [\n",
            "    \"XLMRobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"xlm-roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 250002\n",
            "}\n",
            "\n",
            "loading weights file https://huggingface.co/xlm-roberta-large/resolve/main/pytorch_model.bin from cache at /home/diptesh/.cache/huggingface/transformers/4b3ca85a63804fb7cd317765d9de19ce6208ee0fc9691b209384ee7cfd9cb3b9.64b4693d874c772310b8acda9a1193cfade77d56795a9b488e612f198b68f6f7\n",
            "Some weights of the model checkpoint at xlm-roberta-large were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.weight', 'lm_head.bias']\n",
            "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/home/diptesh/workspace/AggressionDetection-IIITL/Final Notebooks/xlm-roberta-large/xlm-roberta-large-finetuned-code-mixed-DS is already a clone of https://huggingface.co/dipteshkanojia/xlm-roberta-large-finetuned-code-mixed-DS. Make sure you pull the latest changes with `repo.git_pull()`.\n",
            "loading configuration file https://huggingface.co/xlm-roberta-large/resolve/main/config.json from cache at /home/diptesh/.cache/huggingface/transformers/4d7a1550c9ab8701667bc307a1213c040fcc06dc87a5e4994e72aecc0d7e0337.842c7737719967568f4691849854475018d6cf7ce21f52576bb6e0d10091bd3c\n",
            "Model config XLMRobertaConfig {\n",
            "  \"_name_or_path\": \"xlm-roberta-large\",\n",
            "  \"architectures\": [\n",
            "    \"XLMRobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"xlm-roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 250002\n",
            "}\n",
            "\n",
            "loading weights file https://huggingface.co/xlm-roberta-large/resolve/main/pytorch_model.bin from cache at /home/diptesh/.cache/huggingface/transformers/4b3ca85a63804fb7cd317765d9de19ce6208ee0fc9691b209384ee7cfd9cb3b9.64b4693d874c772310b8acda9a1193cfade77d56795a9b488e612f198b68f6f7\n",
            "Some weights of the model checkpoint at xlm-roberta-large were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.weight', 'lm_head.bias']\n",
            "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 3976\n",
            "  Num Epochs = 4\n",
            "  Instantaneous batch size per device = 4\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 1988\n",
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href=\"https://wandb.me/wandb-init\" target=\"_blank\">the W&B docs</a>."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.13.3 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.13.2"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/diptesh/workspace/AggressionDetection-IIITL/Final Notebooks/xlm-roberta-large/wandb/run-20220927_125710-1zzt9wgg</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/diptesh/aggression_detection/runs/1zzt9wgg\" target=\"_blank\">xlmr-large-code-mixed-DS</a></strong> to <a href=\"https://wandb.ai/diptesh/aggression_detection\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/json": {
              "ascii": false,
              "bar_format": null,
              "colour": null,
              "elapsed": 0.024492740631103516,
              "initial": 0,
              "n": 0,
              "ncols": null,
              "nrows": null,
              "postfix": null,
              "prefix": "",
              "rate": null,
              "total": 1988,
              "unit": "it",
              "unit_divisor": 1000,
              "unit_scale": false
            },
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "54de6a654fa84ed2b7a14491d831545e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1988 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 497\n",
            "  Batch size = 16\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 1.098, 'learning_rate': 8.75251509054326e-06, 'epoch': 0.5}\n"
          ]
        },
        {
          "data": {
            "application/json": {
              "ascii": false,
              "bar_format": null,
              "colour": null,
              "elapsed": 0.026255369186401367,
              "initial": 0,
              "n": 0,
              "ncols": null,
              "nrows": null,
              "postfix": null,
              "prefix": "",
              "rate": null,
              "total": 32,
              "unit": "it",
              "unit_divisor": 1000,
              "unit_scale": false
            },
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "df2902d3c4e249148a23c748585c2604",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/32 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Saving model checkpoint to xlm-roberta-large-finetuned-code-mixed-DS/checkpoint-248\n",
            "Configuration saved in xlm-roberta-large-finetuned-code-mixed-DS/checkpoint-248/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 1.0943673849105835, 'eval_accuracy': 0.5352112676056338, 'eval_precision': 0.23546051142792487, 'eval_recall': 0.33438255372274023, 'eval_f1': 0.23973614775725596, 'eval_runtime': 5.4237, 'eval_samples_per_second': 91.634, 'eval_steps_per_second': 5.9, 'epoch': 0.5}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in xlm-roberta-large-finetuned-code-mixed-DS/checkpoint-248/pytorch_model.bin\n",
            "tokenizer config file saved in xlm-roberta-large-finetuned-code-mixed-DS/checkpoint-248/tokenizer_config.json\n",
            "Special tokens file saved in xlm-roberta-large-finetuned-code-mixed-DS/checkpoint-248/special_tokens_map.json\n",
            "tokenizer config file saved in xlm-roberta-large-finetuned-code-mixed-DS/tokenizer_config.json\n",
            "Special tokens file saved in xlm-roberta-large-finetuned-code-mixed-DS/special_tokens_map.json\n",
            "/home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 497\n",
            "  Batch size = 16\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 1.0827, 'learning_rate': 7.5050301810865204e-06, 'epoch': 1.0}\n"
          ]
        },
        {
          "data": {
            "application/json": {
              "ascii": false,
              "bar_format": null,
              "colour": null,
              "elapsed": 0.018978118896484375,
              "initial": 0,
              "n": 0,
              "ncols": null,
              "nrows": null,
              "postfix": null,
              "prefix": "",
              "rate": null,
              "total": 32,
              "unit": "it",
              "unit_divisor": 1000,
              "unit_scale": false
            },
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "66fd60ff5d534f5ebc8828f74829f5f7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/32 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to xlm-roberta-large-finetuned-code-mixed-DS/checkpoint-496\n",
            "Configuration saved in xlm-roberta-large-finetuned-code-mixed-DS/checkpoint-496/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 1.0956871509552002, 'eval_accuracy': 0.5352112676056338, 'eval_precision': 0.5789115646258504, 'eval_recall': 0.3378827545043039, 'eval_f1': 0.2502330494580136, 'eval_runtime': 5.6349, 'eval_samples_per_second': 88.2, 'eval_steps_per_second': 5.679, 'epoch': 1.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in xlm-roberta-large-finetuned-code-mixed-DS/checkpoint-496/pytorch_model.bin\n",
            "tokenizer config file saved in xlm-roberta-large-finetuned-code-mixed-DS/checkpoint-496/tokenizer_config.json\n",
            "Special tokens file saved in xlm-roberta-large-finetuned-code-mixed-DS/checkpoint-496/special_tokens_map.json\n",
            "/home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 497\n",
            "  Batch size = 16\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 1.0503, 'learning_rate': 6.257545271629779e-06, 'epoch': 1.5}\n"
          ]
        },
        {
          "data": {
            "application/json": {
              "ascii": false,
              "bar_format": null,
              "colour": null,
              "elapsed": 0.027627944946289062,
              "initial": 0,
              "n": 0,
              "ncols": null,
              "nrows": null,
              "postfix": null,
              "prefix": "",
              "rate": null,
              "total": 32,
              "unit": "it",
              "unit_divisor": 1000,
              "unit_scale": false
            },
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e43c9c6686744000ae51d88ab8b03e8a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/32 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to xlm-roberta-large-finetuned-code-mixed-DS/checkpoint-744\n",
            "Configuration saved in xlm-roberta-large-finetuned-code-mixed-DS/checkpoint-744/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.9969201683998108, 'eval_accuracy': 0.5311871227364185, 'eval_precision': 0.36208569030725135, 'eval_recall': 0.4996148431481924, 'eval_f1': 0.39137588972572473, 'eval_runtime': 5.5866, 'eval_samples_per_second': 88.962, 'eval_steps_per_second': 5.728, 'epoch': 1.5}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in xlm-roberta-large-finetuned-code-mixed-DS/checkpoint-744/pytorch_model.bin\n",
            "tokenizer config file saved in xlm-roberta-large-finetuned-code-mixed-DS/checkpoint-744/tokenizer_config.json\n",
            "Special tokens file saved in xlm-roberta-large-finetuned-code-mixed-DS/checkpoint-744/special_tokens_map.json\n",
            "/home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 497\n",
            "  Batch size = 16\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.9728, 'learning_rate': 5.010060362173038e-06, 'epoch': 2.0}\n"
          ]
        },
        {
          "data": {
            "application/json": {
              "ascii": false,
              "bar_format": null,
              "colour": null,
              "elapsed": 0.014177322387695312,
              "initial": 0,
              "n": 0,
              "ncols": null,
              "nrows": null,
              "postfix": null,
              "prefix": "",
              "rate": null,
              "total": 32,
              "unit": "it",
              "unit_divisor": 1000,
              "unit_scale": false
            },
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d699ae7180da4da2a92a0f549cf177ca",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/32 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to xlm-roberta-large-finetuned-code-mixed-DS/checkpoint-992\n",
            "Configuration saved in xlm-roberta-large-finetuned-code-mixed-DS/checkpoint-992/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.8525352478027344, 'eval_accuracy': 0.6056338028169014, 'eval_precision': 0.5095861692145055, 'eval_recall': 0.5565225492539833, 'eval_f1': 0.4677701568945072, 'eval_runtime': 5.4196, 'eval_samples_per_second': 91.704, 'eval_steps_per_second': 5.904, 'epoch': 2.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in xlm-roberta-large-finetuned-code-mixed-DS/checkpoint-992/pytorch_model.bin\n",
            "tokenizer config file saved in xlm-roberta-large-finetuned-code-mixed-DS/checkpoint-992/tokenizer_config.json\n",
            "Special tokens file saved in xlm-roberta-large-finetuned-code-mixed-DS/checkpoint-992/special_tokens_map.json\n",
            "tokenizer config file saved in xlm-roberta-large-finetuned-code-mixed-DS/tokenizer_config.json\n",
            "Special tokens file saved in xlm-roberta-large-finetuned-code-mixed-DS/special_tokens_map.json\n",
            "/home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 497\n",
            "  Batch size = 16\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.9271, 'learning_rate': 3.762575452716298e-06, 'epoch': 2.49}\n"
          ]
        },
        {
          "data": {
            "application/json": {
              "ascii": false,
              "bar_format": null,
              "colour": null,
              "elapsed": 0.01557779312133789,
              "initial": 0,
              "n": 0,
              "ncols": null,
              "nrows": null,
              "postfix": null,
              "prefix": "",
              "rate": null,
              "total": 32,
              "unit": "it",
              "unit_divisor": 1000,
              "unit_scale": false
            },
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "acdfcd1db5da4281a405789d93da632d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/32 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to xlm-roberta-large-finetuned-code-mixed-DS/checkpoint-1240\n",
            "Configuration saved in xlm-roberta-large-finetuned-code-mixed-DS/checkpoint-1240/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.780880868434906, 'eval_accuracy': 0.6378269617706237, 'eval_precision': 0.6014450368255834, 'eval_recall': 0.6320293078582936, 'eval_f1': 0.5963419489037292, 'eval_runtime': 5.7541, 'eval_samples_per_second': 86.374, 'eval_steps_per_second': 5.561, 'epoch': 2.49}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in xlm-roberta-large-finetuned-code-mixed-DS/checkpoint-1240/pytorch_model.bin\n",
            "tokenizer config file saved in xlm-roberta-large-finetuned-code-mixed-DS/checkpoint-1240/tokenizer_config.json\n",
            "Special tokens file saved in xlm-roberta-large-finetuned-code-mixed-DS/checkpoint-1240/special_tokens_map.json\n",
            "/home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 497\n",
            "  Batch size = 16\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.7977, 'learning_rate': 2.5150905432595575e-06, 'epoch': 2.99}\n"
          ]
        },
        {
          "data": {
            "application/json": {
              "ascii": false,
              "bar_format": null,
              "colour": null,
              "elapsed": 0.02626776695251465,
              "initial": 0,
              "n": 0,
              "ncols": null,
              "nrows": null,
              "postfix": null,
              "prefix": "",
              "rate": null,
              "total": 32,
              "unit": "it",
              "unit_divisor": 1000,
              "unit_scale": false
            },
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b8973d2db39f45dc8b587db25295745f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/32 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to xlm-roberta-large-finetuned-code-mixed-DS/checkpoint-1488\n",
            "Configuration saved in xlm-roberta-large-finetuned-code-mixed-DS/checkpoint-1488/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.8289645314216614, 'eval_accuracy': 0.5875251509054326, 'eval_precision': 0.5629978668069314, 'eval_recall': 0.5917560027437543, 'eval_f1': 0.5389740698301838, 'eval_runtime': 5.7593, 'eval_samples_per_second': 86.295, 'eval_steps_per_second': 5.556, 'epoch': 2.99}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in xlm-roberta-large-finetuned-code-mixed-DS/checkpoint-1488/pytorch_model.bin\n",
            "tokenizer config file saved in xlm-roberta-large-finetuned-code-mixed-DS/checkpoint-1488/tokenizer_config.json\n",
            "Special tokens file saved in xlm-roberta-large-finetuned-code-mixed-DS/checkpoint-1488/special_tokens_map.json\n",
            "/home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 497\n",
            "  Batch size = 16\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.752, 'learning_rate': 1.267605633802817e-06, 'epoch': 3.49}\n"
          ]
        },
        {
          "data": {
            "application/json": {
              "ascii": false,
              "bar_format": null,
              "colour": null,
              "elapsed": 0.01601552963256836,
              "initial": 0,
              "n": 0,
              "ncols": null,
              "nrows": null,
              "postfix": null,
              "prefix": "",
              "rate": null,
              "total": 32,
              "unit": "it",
              "unit_divisor": 1000,
              "unit_scale": false
            },
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c8fab76d67de47a3af09e6171f435ace",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/32 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to xlm-roberta-large-finetuned-code-mixed-DS/checkpoint-1736\n",
            "Configuration saved in xlm-roberta-large-finetuned-code-mixed-DS/checkpoint-1736/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.7683967351913452, 'eval_accuracy': 0.7122736418511066, 'eval_precision': 0.6526441301983098, 'eval_recall': 0.6610219312842677, 'eval_f1': 0.6557602051068687, 'eval_runtime': 5.5885, 'eval_samples_per_second': 88.933, 'eval_steps_per_second': 5.726, 'epoch': 3.49}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in xlm-roberta-large-finetuned-code-mixed-DS/checkpoint-1736/pytorch_model.bin\n",
            "tokenizer config file saved in xlm-roberta-large-finetuned-code-mixed-DS/checkpoint-1736/tokenizer_config.json\n",
            "Special tokens file saved in xlm-roberta-large-finetuned-code-mixed-DS/checkpoint-1736/special_tokens_map.json\n",
            "/home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 497\n",
            "  Batch size = 16\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.6846, 'learning_rate': 2.0120724346076463e-08, 'epoch': 3.99}\n"
          ]
        },
        {
          "data": {
            "application/json": {
              "ascii": false,
              "bar_format": null,
              "colour": null,
              "elapsed": 0.031078815460205078,
              "initial": 0,
              "n": 0,
              "ncols": null,
              "nrows": null,
              "postfix": null,
              "prefix": "",
              "rate": null,
              "total": 32,
              "unit": "it",
              "unit_divisor": 1000,
              "unit_scale": false
            },
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8a1cc32f512e470291d266453d6d17ea",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/32 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to xlm-roberta-large-finetuned-code-mixed-DS/checkpoint-1984\n",
            "Configuration saved in xlm-roberta-large-finetuned-code-mixed-DS/checkpoint-1984/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.7327519655227661, 'eval_accuracy': 0.7022132796780685, 'eval_precision': 0.6436662965206272, 'eval_recall': 0.6634332631769737, 'eval_f1': 0.6483161727267467, 'eval_runtime': 5.6035, 'eval_samples_per_second': 88.694, 'eval_steps_per_second': 5.711, 'epoch': 3.99}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in xlm-roberta-large-finetuned-code-mixed-DS/checkpoint-1984/pytorch_model.bin\n",
            "tokenizer config file saved in xlm-roberta-large-finetuned-code-mixed-DS/checkpoint-1984/tokenizer_config.json\n",
            "Special tokens file saved in xlm-roberta-large-finetuned-code-mixed-DS/checkpoint-1984/special_tokens_map.json\n",
            "tokenizer config file saved in xlm-roberta-large-finetuned-code-mixed-DS/tokenizer_config.json\n",
            "Special tokens file saved in xlm-roberta-large-finetuned-code-mixed-DS/special_tokens_map.json\n",
            "/home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from xlm-roberta-large-finetuned-code-mixed-DS/checkpoint-1736 (score: 0.6557602051068687).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'train_runtime': 1732.2093, 'train_samples_per_second': 9.181, 'train_steps_per_second': 1.148, 'train_loss': 0.9201524248065603, 'epoch': 4.0}\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0c4e62da34164ecbaa03a3e81e70c5db",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁▁▁▄▅▃██</td></tr><tr><td>eval/f1</td><td>▁▁▄▅▇▆██</td></tr><tr><td>eval/loss</td><td>██▆▃▂▃▂▁</td></tr><tr><td>eval/precision</td><td>▁▇▃▆▇▆██</td></tr><tr><td>eval/recall</td><td>▁▁▅▆▇▆██</td></tr><tr><td>eval/runtime</td><td>▁▅▄▁██▄▅</td></tr><tr><td>eval/samples_per_second</td><td>█▃▄█▁▁▄▄</td></tr><tr><td>eval/steps_per_second</td><td>█▃▄█▁▁▄▄</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▃▃▄▄▅▅▆▆▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▃▃▄▄▅▅▆▆▇▇███</td></tr><tr><td>train/learning_rate</td><td>█▇▆▅▄▃▂▁</td></tr><tr><td>train/loss</td><td>██▇▆▅▃▂▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.70221</td></tr><tr><td>eval/f1</td><td>0.64832</td></tr><tr><td>eval/loss</td><td>0.73275</td></tr><tr><td>eval/precision</td><td>0.64367</td></tr><tr><td>eval/recall</td><td>0.66343</td></tr><tr><td>eval/runtime</td><td>5.6035</td></tr><tr><td>eval/samples_per_second</td><td>88.694</td></tr><tr><td>eval/steps_per_second</td><td>5.711</td></tr><tr><td>train/epoch</td><td>4.0</td></tr><tr><td>train/global_step</td><td>1988</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.6846</td></tr><tr><td>train/total_flos</td><td>1.4821486480883712e+16</td></tr><tr><td>train/train_loss</td><td>0.92015</td></tr><tr><td>train/train_runtime</td><td>1732.2093</td></tr><tr><td>train/train_samples_per_second</td><td>9.181</td></tr><tr><td>train/train_steps_per_second</td><td>1.148</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">xlmr-large-code-mixed-DS</strong>: <a href=\"https://wandb.ai/diptesh/aggression_detection/runs/1zzt9wgg\" target=\"_blank\">https://wandb.ai/diptesh/aggression_detection/runs/1zzt9wgg</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20220927_125710-1zzt9wgg/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "trainer = CustomTrainer(model_init=model_init,\n",
        "                        args=training_args,\n",
        "                        compute_metrics = compute_metrics,\n",
        "                        train_dataset = train_dataset,\n",
        "                        eval_dataset = valid_dataset,\n",
        "                        tokenizer = tokenizer, \n",
        "                        callbacks = [EarlyStoppingCallback(early_stopping_patience = 2, early_stopping_threshold=0.0001)]\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# post-training analysis, testing, other logged code\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "gguBpeh4xGdy"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to xlm-roberta-large-finetuned-code-mixed-DS\n",
            "Configuration saved in xlm-roberta-large-finetuned-code-mixed-DS/config.json\n",
            "Model weights saved in xlm-roberta-large-finetuned-code-mixed-DS/pytorch_model.bin\n",
            "tokenizer config file saved in xlm-roberta-large-finetuned-code-mixed-DS/tokenizer_config.json\n",
            "Special tokens file saved in xlm-roberta-large-finetuned-code-mixed-DS/special_tokens_map.json\n",
            "Several commits (2) will be pushed upstream.\n",
            "The progress bars may be unreliable.\n"
          ]
        },
        {
          "data": {
            "application/json": {
              "ascii": false,
              "bar_format": null,
              "colour": null,
              "elapsed": 0.03446197509765625,
              "initial": 32768,
              "n": 32768,
              "ncols": null,
              "nrows": null,
              "postfix": null,
              "prefix": "Upload file pytorch_model.bin",
              "rate": null,
              "total": 2239711213,
              "unit": "B",
              "unit_divisor": 1024,
              "unit_scale": true
            },
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fecdb576516945b996c20bcd736d37db",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Upload file pytorch_model.bin:   0%|          | 32.0k/2.09G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "remote: Scanning LFS files for validity, may be slow...        \n",
            "remote: LFS file scan complete.        \n",
            "To https://huggingface.co/dipteshkanojia/xlm-roberta-large-finetuned-code-mixed-DS\n",
            "   2b4cc09..898a754  main -> main\n",
            "\n",
            "Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Text Classification', 'type': 'text-classification'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.7022132796780685}, {'name': 'Precision', 'type': 'precision', 'value': 0.6436662965206272}, {'name': 'Recall', 'type': 'recall', 'value': 0.6634332631769737}, {'name': 'F1', 'type': 'f1', 'value': 0.6483161727267467}]}\n",
            "To https://huggingface.co/dipteshkanojia/xlm-roberta-large-finetuned-code-mixed-DS\n",
            "   898a754..64cdc6b  main -> main\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'https://huggingface.co/dipteshkanojia/xlm-roberta-large-finetuned-code-mixed-DS/commit/898a7542c2c17081991f01e5b9751393ee7989aa'"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.push_to_hub()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8DGegrFFB9c"
      },
      "source": [
        "## 8) Predictions and Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "uwWgMmrenkxF"
      },
      "outputs": [],
      "source": [
        "test_texts = list(test_df['Sentence'])\n",
        "test_labels = list(test_df['Label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "J18PaJADvljI"
      },
      "outputs": [],
      "source": [
        "test_encodings = tokenizer(test_texts, truncation=True, padding=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "aFnak-ItvrG-"
      },
      "outputs": [],
      "source": [
        "test_dataset = AggressionDataset(test_encodings, test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "qFi6MZz-g9xu"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Prediction *****\n",
            "  Num examples = 498\n",
            "  Batch size = 16\n",
            "/home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
          ]
        },
        {
          "data": {
            "application/json": {
              "ascii": false,
              "bar_format": null,
              "colour": null,
              "elapsed": 0.014483928680419922,
              "initial": 0,
              "n": 0,
              "ncols": null,
              "nrows": null,
              "postfix": null,
              "prefix": "",
              "rate": null,
              "total": 32,
              "unit": "it",
              "unit_divisor": 1000,
              "unit_scale": false
            },
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d60d9cb970c041cc9e0fb760cfc72c62",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/32 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "preds_output_test = trainer.predict(test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "nQJfQMYWhAtz"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'test_loss': 0.820253849029541,\n",
              " 'test_accuracy': 0.7028112449799196,\n",
              " 'test_precision': 0.6406983480154211,\n",
              " 'test_recall': 0.6487571529087977,\n",
              " 'test_f1': 0.6429957557043565,\n",
              " 'test_runtime': 5.9262,\n",
              " 'test_samples_per_second': 84.033,\n",
              " 'test_steps_per_second': 5.4}"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "preds_output_test.metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "tR_TsEhihCtm"
      },
      "outputs": [],
      "source": [
        "y_preds_test = np.argmax(preds_output_test.predictions, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "PfZhPHNFhE3B"
      },
      "outputs": [],
      "source": [
        "y_valid_test = np.array(test_dataset.labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "dZRj0AV4hGcP"
      },
      "outputs": [],
      "source": [
        "map_dt = {0:'NAG', 1:'CAG', 2:'OAG'}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "sgugEinyhH_X"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         NAG       0.85      0.84      0.84       268\n",
            "         CAG       0.57      0.51      0.54       136\n",
            "         OAG       0.50      0.60      0.55        94\n",
            "\n",
            "    accuracy                           0.70       498\n",
            "   macro avg       0.64      0.65      0.64       498\n",
            "weighted avg       0.71      0.70      0.70       498\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(classification_report(y_valid_test, y_preds_test, target_names=list(map_dt.values())))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "KCcc6g3NhLj7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: seaborn in /home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages (0.12.0)\n",
            "Requirement already satisfied: pandas>=0.25 in /home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages (from seaborn) (1.3.5)\n",
            "Requirement already satisfied: numpy>=1.17 in /home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages (from seaborn) (1.21.6)\n",
            "Requirement already satisfied: matplotlib>=3.1 in /home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages (from seaborn) (3.2.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages (from matplotlib>=3.1->seaborn) (1.4.3)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages (from matplotlib>=3.1->seaborn) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages (from matplotlib>=3.1->seaborn) (3.0.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages (from matplotlib>=3.1->seaborn) (0.11.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages (from pandas>=0.25->seaborn) (2019.3)\n",
            "Requirement already satisfied: six>=1.5 in /home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages (from python-dateutil>=2.1->matplotlib>=3.1->seaborn) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install seaborn\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "y_valid_trying = map(lambda x : map_dt[x], y_valid_test)\n",
        "y_valid_trying = list(y_valid_trying)\n",
        "\n",
        "y_preds_trying = map(lambda x : map_dt[x], y_preds_test)\n",
        "y_preds_trying = list(y_preds_trying)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "IwHUVo5PBE-x"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fd6e0dfaa90>"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "cm_labels = np.unique(y_valid_trying)\n",
        "cm_array = confusion_matrix(y_valid_trying, y_preds_trying)\n",
        "cm_array_df = pd.DataFrame(cm_array, index=cm_labels, columns=cm_labels)\n",
        "sns.heatmap(cm_array_df, annot=True, annot_kws={\"size\": 12}) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.8.13 ('aggDet')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "0c1cc14d24d579ea9f448eff41282ce5bee110707ee119424f8b85e664f18efb"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
