{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "LF0eI-2QE_ky"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tue Sep 13 14:14:59 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.141.03   Driver Version: 470.141.03   CUDA Version: 11.4     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  NVIDIA RTX A5000    Off  | 00000000:17:00.0 Off |                  Off |\n",
            "| 30%   36C    P8    21W / 230W |     10MiB / 24256MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "|   1  NVIDIA RTX A5000    Off  | 00000000:65:00.0  On |                  Off |\n",
            "| 30%   41C    P8    30W / 230W |    413MiB / 24247MiB |     23%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|    0   N/A  N/A      2017      G   /usr/lib/xorg/Xorg                  4MiB |\n",
            "|    0   N/A  N/A      5286      G   /usr/lib/xorg/Xorg                  4MiB |\n",
            "|    1   N/A  N/A      2017      G   /usr/lib/xorg/Xorg                 23MiB |\n",
            "|    1   N/A  N/A      5286      G   /usr/lib/xorg/Xorg                133MiB |\n",
            "|    1   N/A  N/A      5707      G   ...mviewer/tv_bin/TeamViewer       19MiB |\n",
            "|    1   N/A  N/A   3697755      G   ...RendererForSitePerProcess      200MiB |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHu-iZKrS6Tu"
      },
      "source": [
        "## 1) Installing libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "cFQX2EGiXgos"
      },
      "outputs": [],
      "source": [
        "# !pip install transformers\n",
        "# !pip install datasets\n",
        "# !pip install wandb\n",
        "import os\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "le8H055mTeqs"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from datasets import load_dataset, Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0LlF1SVVvNM"
      },
      "source": [
        "## 2) Loading dataset (from HF)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "QPG3xAkTYsw7"
      },
      "outputs": [],
      "source": [
        "# enter your personal read token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "xdJCpTLOXiKv"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b54aade95c884031bb52bf981331f307",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "lTW-jsAmQesI"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using custom data configuration IIIT-L--total_code_mixed-c86de67ddd2696fa\n",
            "Reusing dataset csv (/home/diptesh/.cache/huggingface/datasets/IIIT-L___csv/IIIT-L--total_code_mixed-c86de67ddd2696fa/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58)\n"
          ]
        },
        {
          "data": {
            "application/json": {
              "ascii": false,
              "bar_format": null,
              "colour": null,
              "elapsed": 0.020737648010253906,
              "initial": 0,
              "n": 0,
              "ncols": null,
              "nrows": null,
              "postfix": null,
              "prefix": "",
              "rate": null,
              "total": 3,
              "unit": "it",
              "unit_divisor": 1000,
              "unit_scale": false
            },
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fdf205fdcccd41a4a8102f585a384b7c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['Sentence', 'Label'],\n",
            "        num_rows: 3976\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['Sentence', 'Label'],\n",
            "        num_rows: 498\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['Sentence', 'Label'],\n",
            "        num_rows: 497\n",
            "    })\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "aggression_dataset = load_dataset(\"IIIT-L/total_code_mixed\", use_auth_token=True)\n",
        "\n",
        "print(aggression_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "43SsJM-aTlg7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['Sentence', 'Label'],\n",
              "    num_rows: 3976\n",
              "})"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_ds = aggression_dataset['train']\n",
        "train_ds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7T67guUEX0Nw"
      },
      "source": [
        "## 3) Converting to dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ZxPRh0hUUQz6"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentence</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>We don't spend one hour and rush off from our ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Shivani my classmate</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>True reviewerÃƒÂ°Ã…Â¸Ã¢â€žÂ¢Ã‚Â</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Royal C***yas Bangalore ÃƒÂ°Ã…Â¸Ã‹Å“Ã¢â‚¬Å¡ÃƒÂ...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>*PURE DESH ME AFRA TAFRI KA MAHOOL HAI...INDIA...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            Sentence  Label\n",
              "0  We don't spend one hour and rush off from our ...      1\n",
              "1                               Shivani my classmate      0\n",
              "2                   True reviewerÃƒÂ°Ã…Â¸Ã¢â€žÂ¢Ã‚Â      0\n",
              "3  Royal C***yas Bangalore ÃƒÂ°Ã…Â¸Ã‹Å“Ã¢â‚¬Å¡ÃƒÂ...      2\n",
              "4  *PURE DESH ME AFRA TAFRI KA MAHOOL HAI...INDIA...      2"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "aggression_dataset.set_format(type='pandas')\n",
        "train_df = aggression_dataset['train'][:]\n",
        "valid_df = aggression_dataset['validation'][:]\n",
        "\n",
        "train_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "IJsiywb_ofVt"
      },
      "outputs": [],
      "source": [
        "test_df = aggression_dataset['test'][:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "5LhCKCB4sMCa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0    2137\n",
              "1    1086\n",
              "2     753\n",
              "Name: Label, dtype: int64"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df['Label'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "bqe00WZ_IP2i"
      },
      "outputs": [],
      "source": [
        "# 3976\n",
        "# NAG-CAG-OAG (0-1-2) = 0.54-0.27-0.19"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFKTp8WlXubz"
      },
      "source": [
        "Seeing Distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "9tnlCy6dLRgi"
      },
      "outputs": [],
      "source": [
        "disb_df = train_df.copy(deep=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "wOdtcgKiXLZD"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages/matplotlib/cbook/__init__.py:1376: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  X = np.atleast_1d(X.T if isinstance(X, np.ndarray) else np.asarray(X))\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "disb_df['Words per sentence'] = disb_df['Sentence'].str.split().apply(len)\n",
        "disb_df.boxplot('Words per sentence', by='Label', grid=False, showfliers=False, color='black')\n",
        "plt.suptitle(\"\")\n",
        "plt.xlabel(\"\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5sA4SbW4jmYd"
      },
      "source": [
        "## 4) Tokenizing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "60uCbqkGjo0-"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "CwDB9kRGkG5L"
      },
      "outputs": [],
      "source": [
        "model_ckpt = 'xlm-roberta-large'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-iNzn8oleHo",
        "outputId": "0215f187-91cc-4de9-88f4-af75ecab1cd1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "250002"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.vocab_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "mk_bg4Pat9jw"
      },
      "outputs": [],
      "source": [
        "train_texts = list(train_df['Sentence'])\n",
        "train_labels = list(train_df['Label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "btVmfHrblxqm"
      },
      "outputs": [],
      "source": [
        "valid_texts = list(valid_df['Sentence'])\n",
        "valid_labels = list(valid_df['Label'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8RWWM8sMhyF"
      },
      "source": [
        "## 5) Encoding train-valid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "YV9Fz--nt8X_"
      },
      "outputs": [],
      "source": [
        "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
        "valid_encodings = tokenizer(valid_texts, truncation=True, padding=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Mc6Mpnqbuwx1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "class AggressionDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "mGql29l6ag6N"
      },
      "outputs": [],
      "source": [
        "train_dataset = AggressionDataset(train_encodings, train_labels)\n",
        "valid_dataset = AggressionDataset(valid_encodings, valid_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENs2HmKAanBd"
      },
      "source": [
        "## 6) Setting classification model and evaluation metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "DFmLAL7RbtPe"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "1JfA9ODa83rR"
      },
      "outputs": [],
      "source": [
        "# Use in case of CUDA memory error\n",
        "\n",
        "# import gc\n",
        "\n",
        "# gc.collect()\n",
        "# torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SwnXMX_Hap0V",
        "outputId": "596089c0-7061-4d83-8c0f-573804f42ea9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "num_labels = 3\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "\n",
        "def model_init():\n",
        "    model = (AutoModelForSequenceClassification.from_pretrained(model_ckpt, num_labels=num_labels))\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "IR3ZFBIjcF3H"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, plot_confusion_matrix\n",
        "\n",
        "def compute_metrics(pred):\n",
        "  labels = pred.label_ids\n",
        "  preds = pred.predictions.argmax(-1)\n",
        "\n",
        "  f1 = f1_score(labels, preds, average='macro')\n",
        "  precision = precision_score(labels, preds, average='macro')\n",
        "  recall = recall_score(labels, preds, average='macro')\n",
        "  acc = accuracy_score(labels, preds)\n",
        "  return {'accuracy': acc, 'precision': precision, 'recall': recall, 'f1': f1}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_1OptUlxh9-"
      },
      "source": [
        "## 7) Fine-tuning, visualizing training, saving model to HF  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "KGSxhrQ0vsfs"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdiptesh\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import wandb\n",
        "\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YtVAykzCv7vZ",
        "outputId": "128d694c-7f5c-4e87-82f1-5518427c2ec4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "env: WANDB_PROJECT=aggression_detection\n"
          ]
        }
      ],
      "source": [
        "%env WANDB_PROJECT = aggression_detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "EDakmiAHc150"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer, TrainingArguments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "_LlQYDjndBFG"
      },
      "outputs": [],
      "source": [
        "# Defining hyperparameters\n",
        "eval_batch_size = 16\n",
        "logging_steps = len(train_texts) // eval_batch_size\n",
        "model_name = f\"{model_ckpt}-finetuned-code-mixed-DS\"\n",
        "training_args = TrainingArguments(output_dir=model_name,\n",
        "                                  num_train_epochs=4,\n",
        "                                  learning_rate=4.932923543227153e-05,\n",
        "                                  per_device_train_batch_size=4,\n",
        "                                  per_device_eval_batch_size=8,\n",
        "                                  weight_decay=0.17649239825343255,\n",
        "                                  evaluation_strategy='steps',\n",
        "                                  save_strategy='steps',\n",
        "                                  max_steps=-1,\n",
        "                                  warmup_ratio=0.0,\n",
        "                                  seed=43,\n",
        "                                  data_seed=4,\n",
        "                                  metric_for_best_model=\"eval_f1\",\n",
        "                                  greater_is_better=True,\n",
        "                                  load_best_model_at_end=True, \n",
        "                                  disable_tqdm=False,\n",
        "                                  logging_steps=logging_steps,\n",
        "                                  save_steps=logging_steps,\n",
        "                                  log_level='info', \n",
        "                                  report_to=\"wandb\", \n",
        "                                  run_name=\"xlmr-large-code-mixed-DS\",\n",
        "                                  push_to_hub=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "bC3nDg818V3U"
      },
      "outputs": [],
      "source": [
        "# import gc\n",
        "\n",
        "# gc.collect()\n",
        "# torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "Moy_vPC1XsQ7"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "\n",
        "class CustomTrainer(Trainer):\n",
        "  def compute_loss(self, model, inputs, return_outputs=False):\n",
        "    # device = torch.device('cuda')\n",
        "    # inputs.to(device)\n",
        "    labels = inputs.get(\"labels\")\n",
        "    # forward pass\n",
        "    outputs = model(**inputs)\n",
        "    logits = outputs.get(\"logits\")\n",
        "    # compute custom loss (suppose one has 3 labels with different weights)\n",
        "    loss_fct = nn.CrossEntropyLoss(weight=torch.tensor([0.17, 0.27, 0.56]).to(device))\n",
        "    loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
        "    return (loss, outputs) if return_outputs else loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "a-a-65FPl5YL"
      },
      "outputs": [],
      "source": [
        "from transformers import EarlyStoppingCallback"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "KJDDBeXSoSf5"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f3ab928f07624b3fa65a74bcb8a87b53",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# enter your personal write token here\n",
        "\n",
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "bgj9CC3qeD22"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loading configuration file https://huggingface.co/xlm-roberta-large/resolve/main/config.json from cache at /home/diptesh/.cache/huggingface/transformers/4d7a1550c9ab8701667bc307a1213c040fcc06dc87a5e4994e72aecc0d7e0337.842c7737719967568f4691849854475018d6cf7ce21f52576bb6e0d10091bd3c\n",
            "Model config XLMRobertaConfig {\n",
            "  \"_name_or_path\": \"xlm-roberta-large\",\n",
            "  \"architectures\": [\n",
            "    \"XLMRobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"xlm-roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 250002\n",
            "}\n",
            "\n",
            "loading weights file https://huggingface.co/xlm-roberta-large/resolve/main/pytorch_model.bin from cache at /home/diptesh/.cache/huggingface/transformers/4b3ca85a63804fb7cd317765d9de19ce6208ee0fc9691b209384ee7cfd9cb3b9.64b4693d874c772310b8acda9a1193cfade77d56795a9b488e612f198b68f6f7\n",
            "Some weights of the model checkpoint at xlm-roberta-large were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
            "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Cloning https://huggingface.co/dipteshkanojia/xlm-roberta-large-finetuned-code-mixed-DS into local empty directory.\n",
            "loading configuration file https://huggingface.co/xlm-roberta-large/resolve/main/config.json from cache at /home/diptesh/.cache/huggingface/transformers/4d7a1550c9ab8701667bc307a1213c040fcc06dc87a5e4994e72aecc0d7e0337.842c7737719967568f4691849854475018d6cf7ce21f52576bb6e0d10091bd3c\n",
            "Model config XLMRobertaConfig {\n",
            "  \"_name_or_path\": \"xlm-roberta-large\",\n",
            "  \"architectures\": [\n",
            "    \"XLMRobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"xlm-roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 250002\n",
            "}\n",
            "\n",
            "loading weights file https://huggingface.co/xlm-roberta-large/resolve/main/pytorch_model.bin from cache at /home/diptesh/.cache/huggingface/transformers/4b3ca85a63804fb7cd317765d9de19ce6208ee0fc9691b209384ee7cfd9cb3b9.64b4693d874c772310b8acda9a1193cfade77d56795a9b488e612f198b68f6f7\n",
            "Some weights of the model checkpoint at xlm-roberta-large were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
            "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 3976\n",
            "  Num Epochs = 4\n",
            "  Instantaneous batch size per device = 4\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 1988\n",
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href=\"https://wandb.me/wandb-init\" target=\"_blank\">the W&B docs</a>."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.13.3 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.13.2"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/diptesh/workspace/AggressionDetection-IIITL/Final Notebooks/xlm-roberta-large/wandb/run-20220913_141602-2pvty5q9</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/diptesh/aggression_detection/runs/2pvty5q9\" target=\"_blank\">xlmr-large-code-mixed-DS</a></strong> to <a href=\"https://wandb.ai/diptesh/aggression_detection\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/json": {
              "ascii": false,
              "bar_format": null,
              "colour": null,
              "elapsed": 0.029529333114624023,
              "initial": 0,
              "n": 0,
              "ncols": null,
              "nrows": null,
              "postfix": null,
              "prefix": "",
              "rate": null,
              "total": 1988,
              "unit": "it",
              "unit_divisor": 1000,
              "unit_scale": false
            },
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ff2f59dec16b4091850a85aa5ae6821c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1988 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 497\n",
            "  Batch size = 16\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 1.1183, 'learning_rate': 4.317548775259178e-05, 'epoch': 0.5}\n"
          ]
        },
        {
          "data": {
            "application/json": {
              "ascii": false,
              "bar_format": null,
              "colour": null,
              "elapsed": 0.019107818603515625,
              "initial": 0,
              "n": 0,
              "ncols": null,
              "nrows": null,
              "postfix": null,
              "prefix": "",
              "rate": null,
              "total": 32,
              "unit": "it",
              "unit_divisor": 1000,
              "unit_scale": false
            },
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "27e2d5af2f0647a6a56efe8e01ad541b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/32 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Saving model checkpoint to xlm-roberta-large-finetuned-code-mixed-DS/checkpoint-248\n",
            "Configuration saved in xlm-roberta-large-finetuned-code-mixed-DS/checkpoint-248/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 1.1036680936813354, 'eval_accuracy': 0.27364185110663986, 'eval_precision': 0.09121395036887996, 'eval_recall': 0.3333333333333333, 'eval_f1': 0.14323328067403898, 'eval_runtime': 5.6618, 'eval_samples_per_second': 87.781, 'eval_steps_per_second': 5.652, 'epoch': 0.5}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in xlm-roberta-large-finetuned-code-mixed-DS/checkpoint-248/pytorch_model.bin\n",
            "tokenizer config file saved in xlm-roberta-large-finetuned-code-mixed-DS/checkpoint-248/tokenizer_config.json\n",
            "Special tokens file saved in xlm-roberta-large-finetuned-code-mixed-DS/checkpoint-248/special_tokens_map.json\n",
            "tokenizer config file saved in xlm-roberta-large-finetuned-code-mixed-DS/tokenizer_config.json\n",
            "Special tokens file saved in xlm-roberta-large-finetuned-code-mixed-DS/special_tokens_map.json\n",
            "Adding files tracked by Git LFS: ['tokenizer.json']. This may take a bit of time if the files are large.\n",
            "/home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 497\n",
            "  Batch size = 16\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 1.1195, 'learning_rate': 3.7021740072912035e-05, 'epoch': 1.0}\n"
          ]
        },
        {
          "data": {
            "application/json": {
              "ascii": false,
              "bar_format": null,
              "colour": null,
              "elapsed": 0.014976978302001953,
              "initial": 0,
              "n": 0,
              "ncols": null,
              "nrows": null,
              "postfix": null,
              "prefix": "",
              "rate": null,
              "total": 32,
              "unit": "it",
              "unit_divisor": 1000,
              "unit_scale": false
            },
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a07b50c0e14f45b792bc94882dc44c10",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/32 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Saving model checkpoint to xlm-roberta-large-finetuned-code-mixed-DS/checkpoint-496\n",
            "Configuration saved in xlm-roberta-large-finetuned-code-mixed-DS/checkpoint-496/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 1.1021543741226196, 'eval_accuracy': 0.5372233400402414, 'eval_precision': 0.17907444668008046, 'eval_recall': 0.3333333333333333, 'eval_f1': 0.23298429319371727, 'eval_runtime': 5.5919, 'eval_samples_per_second': 88.878, 'eval_steps_per_second': 5.723, 'epoch': 1.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in xlm-roberta-large-finetuned-code-mixed-DS/checkpoint-496/pytorch_model.bin\n",
            "tokenizer config file saved in xlm-roberta-large-finetuned-code-mixed-DS/checkpoint-496/tokenizer_config.json\n",
            "Special tokens file saved in xlm-roberta-large-finetuned-code-mixed-DS/checkpoint-496/special_tokens_map.json\n",
            "/home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 497\n",
            "  Batch size = 16\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 1.106, 'learning_rate': 3.086799239323229e-05, 'epoch': 1.5}\n"
          ]
        },
        {
          "data": {
            "application/json": {
              "ascii": false,
              "bar_format": null,
              "colour": null,
              "elapsed": 0.0191190242767334,
              "initial": 0,
              "n": 0,
              "ncols": null,
              "nrows": null,
              "postfix": null,
              "prefix": "",
              "rate": null,
              "total": 32,
              "unit": "it",
              "unit_divisor": 1000,
              "unit_scale": false
            },
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ceb2bd72926e4599b4902169c1d57502",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/32 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Saving model checkpoint to xlm-roberta-large-finetuned-code-mixed-DS/checkpoint-744\n",
            "Configuration saved in xlm-roberta-large-finetuned-code-mixed-DS/checkpoint-744/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 1.097253680229187, 'eval_accuracy': 0.1891348088531187, 'eval_precision': 0.0630449362843729, 'eval_recall': 0.3333333333333333, 'eval_f1': 0.10603496897913141, 'eval_runtime': 5.5983, 'eval_samples_per_second': 88.778, 'eval_steps_per_second': 5.716, 'epoch': 1.5}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in xlm-roberta-large-finetuned-code-mixed-DS/checkpoint-744/pytorch_model.bin\n",
            "tokenizer config file saved in xlm-roberta-large-finetuned-code-mixed-DS/checkpoint-744/tokenizer_config.json\n",
            "Special tokens file saved in xlm-roberta-large-finetuned-code-mixed-DS/checkpoint-744/special_tokens_map.json\n",
            "/home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 497\n",
            "  Batch size = 16\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 1.1075, 'learning_rate': 2.4714244713552537e-05, 'epoch': 2.0}\n"
          ]
        },
        {
          "data": {
            "application/json": {
              "ascii": false,
              "bar_format": null,
              "colour": null,
              "elapsed": 0.033738136291503906,
              "initial": 0,
              "n": 0,
              "ncols": null,
              "nrows": null,
              "postfix": null,
              "prefix": "",
              "rate": null,
              "total": 32,
              "unit": "it",
              "unit_divisor": 1000,
              "unit_scale": false
            },
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c10efdac8931412fb447f4d1fdb88ae1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/32 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Saving model checkpoint to xlm-roberta-large-finetuned-code-mixed-DS/checkpoint-992\n",
            "Configuration saved in xlm-roberta-large-finetuned-code-mixed-DS/checkpoint-992/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 1.0927473306655884, 'eval_accuracy': 0.1891348088531187, 'eval_precision': 0.0630449362843729, 'eval_recall': 0.3333333333333333, 'eval_f1': 0.10603496897913141, 'eval_runtime': 5.6136, 'eval_samples_per_second': 88.534, 'eval_steps_per_second': 5.7, 'epoch': 2.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in xlm-roberta-large-finetuned-code-mixed-DS/checkpoint-992/pytorch_model.bin\n",
            "tokenizer config file saved in xlm-roberta-large-finetuned-code-mixed-DS/checkpoint-992/tokenizer_config.json\n",
            "Special tokens file saved in xlm-roberta-large-finetuned-code-mixed-DS/checkpoint-992/special_tokens_map.json\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from xlm-roberta-large-finetuned-code-mixed-DS/checkpoint-496 (score: 0.23298429319371727).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'train_runtime': 529.299, 'train_samples_per_second': 30.047, 'train_steps_per_second': 3.756, 'train_loss': 1.1128156415877803, 'epoch': 2.0}\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1f36a276d2cc485ba602fc75f3725905",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▃█▁▁</td></tr><tr><td>eval/f1</td><td>▃█▁▁</td></tr><tr><td>eval/loss</td><td>█▇▄▁</td></tr><tr><td>eval/precision</td><td>▃█▁▁</td></tr><tr><td>eval/recall</td><td>▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>█▁▂▃</td></tr><tr><td>eval/samples_per_second</td><td>▁█▇▆</td></tr><tr><td>eval/steps_per_second</td><td>▁█▇▆</td></tr><tr><td>train/epoch</td><td>▁▁▃▃▆▆███</td></tr><tr><td>train/global_step</td><td>▁▁▃▃▆▆███</td></tr><tr><td>train/learning_rate</td><td>█▆▃▁</td></tr><tr><td>train/loss</td><td>▇█▁▂</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.18913</td></tr><tr><td>eval/f1</td><td>0.10603</td></tr><tr><td>eval/loss</td><td>1.09275</td></tr><tr><td>eval/precision</td><td>0.06304</td></tr><tr><td>eval/recall</td><td>0.33333</td></tr><tr><td>eval/runtime</td><td>5.6136</td></tr><tr><td>eval/samples_per_second</td><td>88.534</td></tr><tr><td>eval/steps_per_second</td><td>5.7</td></tr><tr><td>train/epoch</td><td>2.0</td></tr><tr><td>train/global_step</td><td>992</td></tr><tr><td>train/learning_rate</td><td>2e-05</td></tr><tr><td>train/loss</td><td>1.1075</td></tr><tr><td>train/total_flos</td><td>7395832288247808.0</td></tr><tr><td>train/train_loss</td><td>1.11282</td></tr><tr><td>train/train_runtime</td><td>529.299</td></tr><tr><td>train/train_samples_per_second</td><td>30.047</td></tr><tr><td>train/train_steps_per_second</td><td>3.756</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">xlmr-large-code-mixed-DS</strong>: <a href=\"https://wandb.ai/diptesh/aggression_detection/runs/2pvty5q9\" target=\"_blank\">https://wandb.ai/diptesh/aggression_detection/runs/2pvty5q9</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20220913_141602-2pvty5q9/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "trainer = CustomTrainer(model_init=model_init,\n",
        "                        args=training_args,\n",
        "                        compute_metrics = compute_metrics,\n",
        "                        train_dataset = train_dataset,\n",
        "                        eval_dataset = valid_dataset,\n",
        "                        tokenizer = tokenizer, \n",
        "                        callbacks = [EarlyStoppingCallback(early_stopping_patience = 2, early_stopping_threshold=0.0001)]\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# post-training analysis, testing, other logged code\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "gguBpeh4xGdy"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to xlm-roberta-large-finetuned-code-mixed-DS\n",
            "Configuration saved in xlm-roberta-large-finetuned-code-mixed-DS/config.json\n",
            "Model weights saved in xlm-roberta-large-finetuned-code-mixed-DS/pytorch_model.bin\n",
            "tokenizer config file saved in xlm-roberta-large-finetuned-code-mixed-DS/tokenizer_config.json\n",
            "Special tokens file saved in xlm-roberta-large-finetuned-code-mixed-DS/special_tokens_map.json\n"
          ]
        },
        {
          "data": {
            "application/json": {
              "ascii": false,
              "bar_format": null,
              "colour": null,
              "elapsed": 0.03889012336730957,
              "initial": 32768,
              "n": 32768,
              "ncols": null,
              "nrows": null,
              "postfix": null,
              "prefix": "Upload file pytorch_model.bin",
              "rate": null,
              "total": 2239711213,
              "unit": "B",
              "unit_divisor": 1024,
              "unit_scale": true
            },
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "647586f3eaa749a1a5e2bc94693cfbda",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Upload file pytorch_model.bin:   0%|          | 32.0k/2.09G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "remote: Scanning LFS files for validity, may be slow...        \n",
            "remote: LFS file scan complete.        \n",
            "To https://huggingface.co/dipteshkanojia/xlm-roberta-large-finetuned-code-mixed-DS\n",
            "   0a0150c..466e52c  main -> main\n",
            "\n",
            "Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Text Classification', 'type': 'text-classification'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.1891348088531187}, {'name': 'Precision', 'type': 'precision', 'value': 0.0630449362843729}, {'name': 'Recall', 'type': 'recall', 'value': 0.3333333333333333}, {'name': 'F1', 'type': 'f1', 'value': 0.10603496897913141}]}\n",
            "To https://huggingface.co/dipteshkanojia/xlm-roberta-large-finetuned-code-mixed-DS\n",
            "   466e52c..edabb65  main -> main\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'https://huggingface.co/dipteshkanojia/xlm-roberta-large-finetuned-code-mixed-DS/commit/466e52c934f2291e4a87bc3f410ecccae1ad22d9'"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.push_to_hub()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8DGegrFFB9c"
      },
      "source": [
        "## 8) Predictions and Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "uwWgMmrenkxF"
      },
      "outputs": [],
      "source": [
        "test_texts = list(test_df['Sentence'])\n",
        "test_labels = list(test_df['Label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "J18PaJADvljI"
      },
      "outputs": [],
      "source": [
        "test_encodings = tokenizer(test_texts, truncation=True, padding=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "aFnak-ItvrG-"
      },
      "outputs": [],
      "source": [
        "test_dataset = AggressionDataset(test_encodings, test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "qFi6MZz-g9xu"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Prediction *****\n",
            "  Num examples = 498\n",
            "  Batch size = 16\n",
            "/home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
          ]
        },
        {
          "data": {
            "application/json": {
              "ascii": false,
              "bar_format": null,
              "colour": null,
              "elapsed": 0.016715526580810547,
              "initial": 0,
              "n": 0,
              "ncols": null,
              "nrows": null,
              "postfix": null,
              "prefix": "",
              "rate": null,
              "total": 32,
              "unit": "it",
              "unit_divisor": 1000,
              "unit_scale": false
            },
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "63b94123736f41c7b31cb763460fb530",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/32 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "preds_output_test = trainer.predict(test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "nQJfQMYWhAtz"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'test_loss': 1.1014723777770996,\n",
              " 'test_accuracy': 0.5381526104417671,\n",
              " 'test_precision': 0.17938420348058903,\n",
              " 'test_recall': 0.3333333333333333,\n",
              " 'test_f1': 0.23324630113141864,\n",
              " 'test_runtime': 6.0888,\n",
              " 'test_samples_per_second': 81.789,\n",
              " 'test_steps_per_second': 5.256}"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "preds_output_test.metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "tR_TsEhihCtm"
      },
      "outputs": [],
      "source": [
        "y_preds_test = np.argmax(preds_output_test.predictions, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "PfZhPHNFhE3B"
      },
      "outputs": [],
      "source": [
        "y_valid_test = np.array(test_dataset.labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "dZRj0AV4hGcP"
      },
      "outputs": [],
      "source": [
        "map_dt = {0:'NAG', 1:'CAG', 2:'OAG'}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "sgugEinyhH_X"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         NAG       0.54      1.00      0.70       268\n",
            "         CAG       0.00      0.00      0.00       136\n",
            "         OAG       0.00      0.00      0.00        94\n",
            "\n",
            "    accuracy                           0.54       498\n",
            "   macro avg       0.18      0.33      0.23       498\n",
            "weighted avg       0.29      0.54      0.38       498\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "print(classification_report(y_valid_test, y_preds_test, target_names=list(map_dt.values())))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "KCcc6g3NhLj7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: seaborn in /home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages (0.12.0)\n",
            "Requirement already satisfied: pandas>=0.25 in /home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages (from seaborn) (1.3.5)\n",
            "Requirement already satisfied: numpy>=1.17 in /home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages (from seaborn) (1.21.6)\n",
            "Requirement already satisfied: matplotlib>=3.1 in /home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages (from seaborn) (3.2.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages (from matplotlib>=3.1->seaborn) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages (from matplotlib>=3.1->seaborn) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages (from matplotlib>=3.1->seaborn) (1.4.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages (from matplotlib>=3.1->seaborn) (0.11.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages (from pandas>=0.25->seaborn) (2019.3)\n",
            "Requirement already satisfied: six>=1.5 in /home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages (from python-dateutil>=2.1->matplotlib>=3.1->seaborn) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install seaborn\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "y_valid_trying = map(lambda x : map_dt[x], y_valid_test)\n",
        "y_valid_trying = list(y_valid_trying)\n",
        "\n",
        "y_preds_trying = map(lambda x : map_dt[x], y_preds_test)\n",
        "y_preds_trying = list(y_preds_trying)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "IwHUVo5PBE-x"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f7bf98a3070>"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "cm_labels = np.unique(y_valid_trying)\n",
        "cm_array = confusion_matrix(y_valid_trying, y_preds_trying)\n",
        "cm_array_df = pd.DataFrame(cm_array, index=cm_labels, columns=cm_labels)\n",
        "sns.heatmap(cm_array_df, annot=True, annot_kws={\"size\": 12}) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.8.13 ('aggDet')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "0c1cc14d24d579ea9f448eff41282ce5bee110707ee119424f8b85e664f18efb"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
