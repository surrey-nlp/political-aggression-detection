{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "LF0eI-2QE_ky"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wed Sep  7 22:49:29 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.141.03   Driver Version: 470.141.03   CUDA Version: 11.4     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  NVIDIA RTX A5000    Off  | 00000000:17:00.0 Off |                  Off |\n",
            "| 30%   38C    P8    21W / 230W |     10MiB / 24256MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "|   1  NVIDIA RTX A5000    Off  | 00000000:65:00.0  On |                  Off |\n",
            "| 30%   44C    P8    31W / 230W |    453MiB / 24247MiB |     39%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|    0   N/A  N/A      2017      G   /usr/lib/xorg/Xorg                  4MiB |\n",
            "|    0   N/A  N/A      5286      G   /usr/lib/xorg/Xorg                  4MiB |\n",
            "|    1   N/A  N/A      2017      G   /usr/lib/xorg/Xorg                 23MiB |\n",
            "|    1   N/A  N/A      5286      G   /usr/lib/xorg/Xorg                224MiB |\n",
            "|    1   N/A  N/A      5707      G   ...mviewer/tv_bin/TeamViewer       15MiB |\n",
            "|    1   N/A  N/A     62482      G   ...RendererForSitePerProcess      152MiB |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHu-iZKrS6Tu"
      },
      "source": [
        "## 1) Installing libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "cFQX2EGiXgos"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages (4.20.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages (from transformers) (2022.7.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: filelock in /home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages (from transformers) (3.7.1)\n",
            "Requirement already satisfied: requests in /home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages (from transformers) (0.8.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: tqdm>=4.27 in /home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.3.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages (from requests->transformers) (1.26.10)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages (from requests->transformers) (3.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages (from requests->transformers) (2022.6.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: datasets in /home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages (2.3.2)\n",
            "Requirement already satisfied: xxhash in /home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages (from datasets) (3.0.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages (from datasets) (2.27.1)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages (from datasets) (8.0.0)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages (from datasets) (2022.5.0)\n",
            "Requirement already satisfied: packaging in /home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages (from datasets) (21.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages (from datasets) (4.64.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages (from datasets) (1.21.6)\n",
            "Requirement already satisfied: responses<0.19 in /home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages (from datasets) (0.18.0)\n",
            "Requirement already satisfied: pandas in /home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages (from datasets) (1.3.5)\n",
            "Requirement already satisfied: multiprocess in /home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages (from datasets) (0.70.13)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages (from datasets) (0.8.1)\n",
            "Requirement already satisfied: aiohttp in /home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages (from datasets) (3.8.1)\n",
            "Requirement already satisfied: dill<0.3.6 in /home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages (from datasets) (0.3.5.1)\n",
            "Requirement already satisfied: filelock in /home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.7.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.3.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages (from packaging->datasets) (3.0.9)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (3.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (2022.6.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (1.26.10)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages (from aiohttp->datasets) (6.0.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages (from aiohttp->datasets) (1.2.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages (from aiohttp->datasets) (21.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages (from aiohttp->datasets) (1.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages (from aiohttp->datasets) (1.7.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages (from pandas->datasets) (2019.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages (from pandas->datasets) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.16.0)\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.13.2-py2.py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages (from wandb) (2.27.1)\n",
            "Requirement already satisfied: protobuf<4.0dev,>=3.12.0 in /home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages (from wandb) (3.20.1)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages (from wandb) (8.0.4)\n",
            "Collecting promise<3,>=2.0\n",
            "  Using cached promise-2.3-py3-none-any.whl\n",
            "Requirement already satisfied: psutil>=5.0.0 in /home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages (from wandb) (5.9.1)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages (from wandb) (1.7.0)\n",
            "Collecting pathtools\n",
            "  Using cached pathtools-0.1.2-py3-none-any.whl\n",
            "Collecting GitPython>=1.0.0\n",
            "  Using cached GitPython-3.1.27-py3-none-any.whl (181 kB)\n",
            "Collecting setproctitle\n",
            "  Downloading setproctitle-1.3.2-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31 kB)\n",
            "Requirement already satisfied: setuptools in /home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages (from wandb) (61.2.0)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Using cached shortuuid-1.0.9-py3-none-any.whl (9.4 kB)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Using cached docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: PyYAML in /home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages (from wandb) (6.0)\n",
            "Requirement already satisfied: six>=1.13.0 in /home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages (from wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Using cached gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages (from requests<3,>=2.0.0->wandb) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages (from requests<3,>=2.0.0->wandb) (3.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages (from requests<3,>=2.0.0->wandb) (2022.6.15)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages (from requests<3,>=2.0.0->wandb) (1.26.10)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Using cached smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: pathtools, smmap, shortuuid, setproctitle, promise, docker-pycreds, gitdb, GitPython, wandb\n",
            "Successfully installed GitPython-3.1.27 docker-pycreds-0.4.0 gitdb-4.0.9 pathtools-0.1.2 promise-2.3 setproctitle-1.3.2 shortuuid-1.0.9 smmap-5.0.0 wandb-0.13.2\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "le8H055mTeqs"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from datasets import load_dataset, Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0LlF1SVVvNM"
      },
      "source": [
        "## 2) Loading dataset (from HF)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "QPG3xAkTYsw7"
      },
      "outputs": [],
      "source": [
        "# enter your personal read token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "xdJCpTLOXiKv"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "20694a5e40f14c6a9f4145c9dfa232e7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "lTW-jsAmQesI"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using custom data configuration IIIT-L--total_code_mixed-c86de67ddd2696fa\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading and preparing dataset csv/IIIT-L--total_code_mixed to /home/diptesh/.cache/huggingface/datasets/IIIT-L___csv/IIIT-L--total_code_mixed-c86de67ddd2696fa/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading data: 100%|██████████| 493k/493k [00:00<00:00, 1.15MB/s]\n",
            "Downloading data: 100%|██████████| 58.9k/58.9k [00:00<00:00, 345kB/s]\n",
            "Downloading data: 100%|██████████| 58.0k/58.0k [00:00<00:00, 345kB/s]\n",
            "Downloading data files: 100%|██████████| 3/3 [00:03<00:00,  1.05s/it]\n",
            "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 837.86it/s]\n",
            "                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset csv downloaded and prepared to /home/diptesh/.cache/huggingface/datasets/IIIT-L___csv/IIIT-L--total_code_mixed-c86de67ddd2696fa/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58. Subsequent calls will reuse this data.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3/3 [00:00<00:00, 110.98it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['Sentence', 'Label'],\n",
            "        num_rows: 3976\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['Sentence', 'Label'],\n",
            "        num_rows: 498\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['Sentence', 'Label'],\n",
            "        num_rows: 497\n",
            "    })\n",
            "})\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "aggression_dataset = load_dataset(\"IIIT-L/total_code_mixed\", use_auth_token=True)\n",
        "\n",
        "print(aggression_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "43SsJM-aTlg7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['Sentence', 'Label'],\n",
              "    num_rows: 3976\n",
              "})"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_ds = aggression_dataset['train']\n",
        "train_ds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7T67guUEX0Nw"
      },
      "source": [
        "## 3) Converting to dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ZxPRh0hUUQz6"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentence</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>We don't spend one hour and rush off from our ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Shivani my classmate</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>True reviewerÃƒÂ°Ã…Â¸Ã¢â€žÂ¢Ã‚Â</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Royal C***yas Bangalore ÃƒÂ°Ã…Â¸Ã‹Å“Ã¢â‚¬Å¡ÃƒÂ...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>*PURE DESH ME AFRA TAFRI KA MAHOOL HAI...INDIA...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            Sentence  Label\n",
              "0  We don't spend one hour and rush off from our ...      1\n",
              "1                               Shivani my classmate      0\n",
              "2                   True reviewerÃƒÂ°Ã…Â¸Ã¢â€žÂ¢Ã‚Â      0\n",
              "3  Royal C***yas Bangalore ÃƒÂ°Ã…Â¸Ã‹Å“Ã¢â‚¬Å¡ÃƒÂ...      2\n",
              "4  *PURE DESH ME AFRA TAFRI KA MAHOOL HAI...INDIA...      2"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "aggression_dataset.set_format(type='pandas')\n",
        "train_df = aggression_dataset['train'][:]\n",
        "valid_df = aggression_dataset['validation'][:]\n",
        "\n",
        "train_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "IJsiywb_ofVt"
      },
      "outputs": [],
      "source": [
        "test_df = aggression_dataset['test'][:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "5LhCKCB4sMCa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0    2137\n",
              "1    1086\n",
              "2     753\n",
              "Name: Label, dtype: int64"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df['Label'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "bqe00WZ_IP2i"
      },
      "outputs": [],
      "source": [
        "# 3976\n",
        "# NAG-CAG-OAG (0-1-2) = 0.54-0.27-0.19"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFKTp8WlXubz"
      },
      "source": [
        "Seeing Distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "9tnlCy6dLRgi"
      },
      "outputs": [],
      "source": [
        "disb_df = train_df.copy(deep=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "wOdtcgKiXLZD"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages/matplotlib/cbook/__init__.py:1376: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  X = np.atleast_1d(X.T if isinstance(X, np.ndarray) else np.asarray(X))\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "disb_df['Words per sentence'] = disb_df['Sentence'].str.split().apply(len)\n",
        "disb_df.boxplot('Words per sentence', by='Label', grid=False, showfliers=False, color='black')\n",
        "plt.suptitle(\"\")\n",
        "plt.xlabel(\"\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5sA4SbW4jmYd"
      },
      "source": [
        "## 4) Tokenizing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "60uCbqkGjo0-"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "CwDB9kRGkG5L"
      },
      "outputs": [],
      "source": [
        "model_ckpt = 'xlm-roberta-base'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-iNzn8oleHo",
        "outputId": "0215f187-91cc-4de9-88f4-af75ecab1cd1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "250002"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.vocab_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "mk_bg4Pat9jw"
      },
      "outputs": [],
      "source": [
        "train_texts = list(train_df['Sentence'])\n",
        "train_labels = list(train_df['Label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "btVmfHrblxqm"
      },
      "outputs": [],
      "source": [
        "valid_texts = list(valid_df['Sentence'])\n",
        "valid_labels = list(valid_df['Label'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8RWWM8sMhyF"
      },
      "source": [
        "## 5) Encoding train-valid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "YV9Fz--nt8X_"
      },
      "outputs": [],
      "source": [
        "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
        "valid_encodings = tokenizer(valid_texts, truncation=True, padding=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "Mc6Mpnqbuwx1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "class AggressionDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "mGql29l6ag6N"
      },
      "outputs": [],
      "source": [
        "train_dataset = AggressionDataset(train_encodings, train_labels)\n",
        "valid_dataset = AggressionDataset(valid_encodings, valid_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENs2HmKAanBd"
      },
      "source": [
        "## 6) Setting classification model and evaluation metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "DFmLAL7RbtPe"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "1JfA9ODa83rR"
      },
      "outputs": [],
      "source": [
        "# Use in case of CUDA memory error\n",
        "\n",
        "# import gc\n",
        "\n",
        "# gc.collect()\n",
        "# torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SwnXMX_Hap0V",
        "outputId": "596089c0-7061-4d83-8c0f-573804f42ea9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "num_labels = 3\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "\n",
        "def model_init():\n",
        "    model = (AutoModelForSequenceClassification.from_pretrained(model_ckpt, num_labels=num_labels))\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "IR3ZFBIjcF3H"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, plot_confusion_matrix\n",
        "\n",
        "def compute_metrics(pred):\n",
        "  labels = pred.label_ids\n",
        "  preds = pred.predictions.argmax(-1)\n",
        "\n",
        "  f1 = f1_score(labels, preds, average='macro')\n",
        "  precision = precision_score(labels, preds, average='macro')\n",
        "  recall = recall_score(labels, preds, average='macro')\n",
        "  acc = accuracy_score(labels, preds)\n",
        "  return {'accuracy': acc, 'precision': precision, 'recall': recall, 'f1': f1}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_1OptUlxh9-"
      },
      "source": [
        "## 7) Fine-tuning, visualizing training, saving model to HF  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "KGSxhrQ0vsfs"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdiptesh\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import wandb\n",
        "\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YtVAykzCv7vZ",
        "outputId": "128d694c-7f5c-4e87-82f1-5518427c2ec4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "env: WANDB_PROJECT=aggression_detection\n"
          ]
        }
      ],
      "source": [
        "%env WANDB_PROJECT = aggression_detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "EDakmiAHc150"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer, TrainingArguments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "_LlQYDjndBFG"
      },
      "outputs": [],
      "source": [
        "# Defining hyperparameters\n",
        "eval_batch_size = 16\n",
        "logging_steps = len(train_texts) // eval_batch_size\n",
        "model_name = f\"{model_ckpt}-finetuned-code-mixed-DS\"\n",
        "training_args = TrainingArguments(output_dir=model_name,\n",
        "                                  num_train_epochs=4,\n",
        "                                  learning_rate=4.932923543227153e-05,\n",
        "                                  per_device_train_batch_size=8,\n",
        "                                  per_device_eval_batch_size=16,\n",
        "                                  weight_decay=0.17649239825343255,\n",
        "                                  evaluation_strategy='steps',\n",
        "                                  save_strategy='steps',\n",
        "                                  max_steps=-1,\n",
        "                                  warmup_ratio=0.0,\n",
        "                                  seed=43,\n",
        "                                  data_seed=4,\n",
        "                                  metric_for_best_model=\"eval_f1\",\n",
        "                                  greater_is_better=True,\n",
        "                                  load_best_model_at_end=True, \n",
        "                                  disable_tqdm=False,\n",
        "                                  logging_steps=logging_steps,\n",
        "                                  save_steps=logging_steps,\n",
        "                                  log_level='info', \n",
        "                                  report_to=\"wandb\", \n",
        "                                  run_name=\"xlmr-code-mixed-DS\",\n",
        "                                  push_to_hub=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "bC3nDg818V3U"
      },
      "outputs": [],
      "source": [
        "# import gc\n",
        "\n",
        "# gc.collect()\n",
        "# torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "Moy_vPC1XsQ7"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "\n",
        "class CustomTrainer(Trainer):\n",
        "  def compute_loss(self, model, inputs, return_outputs=False):\n",
        "    # device = torch.device('cuda')\n",
        "    # inputs.to(device)\n",
        "    labels = inputs.get(\"labels\")\n",
        "    # forward pass\n",
        "    outputs = model(**inputs)\n",
        "    logits = outputs.get(\"logits\")\n",
        "    # compute custom loss (suppose one has 3 labels with different weights)\n",
        "    loss_fct = nn.CrossEntropyLoss(weight=torch.tensor([0.17, 0.27, 0.56]).to(device))\n",
        "    loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
        "    return (loss, outputs) if return_outputs else loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "a-a-65FPl5YL"
      },
      "outputs": [],
      "source": [
        "from transformers import EarlyStoppingCallback"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "KJDDBeXSoSf5"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1afac80bd81d4c88abaee3234c04940a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# enter your personal write token here\n",
        "\n",
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "bgj9CC3qeD22"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loading configuration file https://huggingface.co/xlm-roberta-base/resolve/main/config.json from cache at /home/diptesh/.cache/huggingface/transformers/87683eb92ea383b0475fecf99970e950a03c9ff5e51648d6eee56fb754612465.dfaaaedc7c1c475302398f09706cbb21e23951b73c6e2b3162c1c8a99bb3b62a\n",
            "Model config XLMRobertaConfig {\n",
            "  \"_name_or_path\": \"xlm-roberta-base\",\n",
            "  \"architectures\": [\n",
            "    \"XLMRobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"xlm-roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 250002\n",
            "}\n",
            "\n",
            "loading weights file https://huggingface.co/xlm-roberta-base/resolve/main/pytorch_model.bin from cache at /home/diptesh/.cache/huggingface/transformers/97d0ea09f8074264957d062ec20ccb79af7b917d091add8261b26874daf51b5d.f42212747c1c27fcebaa0a89e2a83c38c6d3d4340f21922f892b88d882146ac2\n",
            "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/diptesh/workspace/AggressionDetection-IIITL/Final Notebooks/xlm-roberta-base/xlm-roberta-base-finetuned-code-mixed-DS is already a clone of https://huggingface.co/dipteshkanojia/xlm-roberta-base-finetuned-code-mixed-DS. Make sure you pull the latest changes with `repo.git_pull()`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loading configuration file https://huggingface.co/xlm-roberta-base/resolve/main/config.json from cache at /home/diptesh/.cache/huggingface/transformers/87683eb92ea383b0475fecf99970e950a03c9ff5e51648d6eee56fb754612465.dfaaaedc7c1c475302398f09706cbb21e23951b73c6e2b3162c1c8a99bb3b62a\n",
            "Model config XLMRobertaConfig {\n",
            "  \"_name_or_path\": \"xlm-roberta-base\",\n",
            "  \"architectures\": [\n",
            "    \"XLMRobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"xlm-roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 250002\n",
            "}\n",
            "\n",
            "loading weights file https://huggingface.co/xlm-roberta-base/resolve/main/pytorch_model.bin from cache at /home/diptesh/.cache/huggingface/transformers/97d0ea09f8074264957d062ec20ccb79af7b917d091add8261b26874daf51b5d.f42212747c1c27fcebaa0a89e2a83c38c6d3d4340f21922f892b88d882146ac2\n",
            "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 3976\n",
            "  Num Epochs = 4\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 996\n",
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "/home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "\n",
            "  0%|          | 0/996 [19:17:14<?, ?it/s]       ***** Running Evaluation *****\n",
            "  Num examples = 497\n",
            "  Batch size = 32\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 1.0602, 'learning_rate': 3.7046453919015166e-05, 'epoch': 1.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "                                                 \n",
            "\n",
            "  0%|          | 0/996 [19:17:16<?, ?it/s]     \n",
            "\u001b[A\n",
            "\u001b[ASaving model checkpoint to xlm-roberta-base-finetuned-code-mixed-DS/checkpoint-248\n",
            "Configuration saved in xlm-roberta-base-finetuned-code-mixed-DS/checkpoint-248/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 1.028007984161377, 'eval_accuracy': 0.5211267605633803, 'eval_precision': 0.4094700464998462, 'eval_recall': 0.4557298136403338, 'eval_f1': 0.39121997960144, 'eval_runtime': 1.8592, 'eval_samples_per_second': 267.315, 'eval_steps_per_second': 8.606, 'epoch': 1.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in xlm-roberta-base-finetuned-code-mixed-DS/checkpoint-248/pytorch_model.bin\n",
            "tokenizer config file saved in xlm-roberta-base-finetuned-code-mixed-DS/checkpoint-248/tokenizer_config.json\n",
            "Special tokens file saved in xlm-roberta-base-finetuned-code-mixed-DS/checkpoint-248/special_tokens_map.json\n",
            "tokenizer config file saved in xlm-roberta-base-finetuned-code-mixed-DS/tokenizer_config.json\n",
            "Special tokens file saved in xlm-roberta-base-finetuned-code-mixed-DS/special_tokens_map.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Adding files tracked by Git LFS: ['tokenizer.json']. This may take a bit of time if the files are large.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "\n",
            "  0%|          | 0/996 [19:18:59<?, ?it/s]       ***** Running Evaluation *****\n",
            "  Num examples = 497\n",
            "  Batch size = 32\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.9741, 'learning_rate': 2.4763672405758802e-05, 'epoch': 1.99}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "                                                 \n",
            "\n",
            "  0%|          | 0/996 [19:19:01<?, ?it/s]     \n",
            "\u001b[A\n",
            "\u001b[ASaving model checkpoint to xlm-roberta-base-finetuned-code-mixed-DS/checkpoint-496\n",
            "Configuration saved in xlm-roberta-base-finetuned-code-mixed-DS/checkpoint-496/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.9318119287490845, 'eval_accuracy': 0.5533199195171026, 'eval_precision': 0.47575367516024847, 'eval_recall': 0.5002062503222661, 'eval_f1': 0.4415132020470671, 'eval_runtime': 1.9683, 'eval_samples_per_second': 252.499, 'eval_steps_per_second': 8.129, 'epoch': 1.99}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in xlm-roberta-base-finetuned-code-mixed-DS/checkpoint-496/pytorch_model.bin\n",
            "tokenizer config file saved in xlm-roberta-base-finetuned-code-mixed-DS/checkpoint-496/tokenizer_config.json\n",
            "Special tokens file saved in xlm-roberta-base-finetuned-code-mixed-DS/checkpoint-496/special_tokens_map.json\n",
            "/home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "\n",
            "  0%|          | 0/996 [19:20:14<?, ?it/s]       ***** Running Evaluation *****\n",
            "  Num examples = 497\n",
            "  Batch size = 32\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.8585, 'learning_rate': 1.2480890892502435e-05, 'epoch': 2.99}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "                                                 \n",
            "\n",
            "  0%|          | 0/996 [19:20:16<?, ?it/s]     \n",
            "\u001b[A\n",
            "\u001b[ASaving model checkpoint to xlm-roberta-base-finetuned-code-mixed-DS/checkpoint-744\n",
            "Configuration saved in xlm-roberta-base-finetuned-code-mixed-DS/checkpoint-744/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.8584890961647034, 'eval_accuracy': 0.607645875251509, 'eval_precision': 0.5539103758931345, 'eval_recall': 0.5730608563450881, 'eval_f1': 0.5353213396127181, 'eval_runtime': 1.7594, 'eval_samples_per_second': 282.489, 'eval_steps_per_second': 9.094, 'epoch': 2.99}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in xlm-roberta-base-finetuned-code-mixed-DS/checkpoint-744/pytorch_model.bin\n",
            "tokenizer config file saved in xlm-roberta-base-finetuned-code-mixed-DS/checkpoint-744/tokenizer_config.json\n",
            "Special tokens file saved in xlm-roberta-base-finetuned-code-mixed-DS/checkpoint-744/special_tokens_map.json\n",
            "/home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "\n",
            "  0%|          | 0/996 [19:21:29<?, ?it/s]       ***** Running Evaluation *****\n",
            "  Num examples = 497\n",
            "  Batch size = 32\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.7293, 'learning_rate': 1.981093792460704e-07, 'epoch': 3.98}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "                                                 \n",
            "\n",
            "  0%|          | 0/996 [19:21:31<?, ?it/s]     \n",
            "\u001b[A\n",
            "\u001b[ASaving model checkpoint to xlm-roberta-base-finetuned-code-mixed-DS/checkpoint-992\n",
            "Configuration saved in xlm-roberta-base-finetuned-code-mixed-DS/checkpoint-992/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.8266023397445679, 'eval_accuracy': 0.6317907444668008, 'eval_precision': 0.578065000258759, 'eval_recall': 0.5977698793279365, 'eval_f1': 0.5676661838384218, 'eval_runtime': 1.8154, 'eval_samples_per_second': 273.775, 'eval_steps_per_second': 8.814, 'epoch': 3.98}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in xlm-roberta-base-finetuned-code-mixed-DS/checkpoint-992/pytorch_model.bin\n",
            "tokenizer config file saved in xlm-roberta-base-finetuned-code-mixed-DS/checkpoint-992/tokenizer_config.json\n",
            "Special tokens file saved in xlm-roberta-base-finetuned-code-mixed-DS/checkpoint-992/special_tokens_map.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "tokenizer config file saved in xlm-roberta-base-finetuned-code-mixed-DS/tokenizer_config.json\n",
            "Special tokens file saved in xlm-roberta-base-finetuned-code-mixed-DS/special_tokens_map.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from xlm-roberta-base-finetuned-code-mixed-DS/checkpoint-992 (score: 0.5676661838384218).\n",
            "\n",
            "100%|██████████| 996/996 [07:43<00:00,  2.15it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'train_runtime': 463.4362, 'train_samples_per_second': 34.318, 'train_steps_per_second': 2.149, 'train_loss': 0.9052019868509836, 'epoch': 4.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cfaf12ebf9da41a6957aaec076b0abcd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁▃▆█</td></tr><tr><td>eval/f1</td><td>▁▃▇█</td></tr><tr><td>eval/loss</td><td>█▅▂▁</td></tr><tr><td>eval/precision</td><td>▁▄▇█</td></tr><tr><td>eval/recall</td><td>▁▃▇█</td></tr><tr><td>eval/runtime</td><td>▄█▁▃</td></tr><tr><td>eval/samples_per_second</td><td>▄▁█▆</td></tr><tr><td>eval/steps_per_second</td><td>▄▁█▆</td></tr><tr><td>train/epoch</td><td>▁▁▃▃▆▆███</td></tr><tr><td>train/global_step</td><td>▁▁▃▃▆▆███</td></tr><tr><td>train/learning_rate</td><td>█▆▃▁</td></tr><tr><td>train/loss</td><td>█▆▄▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.63179</td></tr><tr><td>eval/f1</td><td>0.56767</td></tr><tr><td>eval/loss</td><td>0.8266</td></tr><tr><td>eval/precision</td><td>0.57807</td></tr><tr><td>eval/recall</td><td>0.59777</td></tr><tr><td>eval/runtime</td><td>1.8154</td></tr><tr><td>eval/samples_per_second</td><td>273.775</td></tr><tr><td>eval/steps_per_second</td><td>8.814</td></tr><tr><td>train/epoch</td><td>4.0</td></tr><tr><td>train/global_step</td><td>996</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.7293</td></tr><tr><td>train/total_flos</td><td>4184555795546112.0</td></tr><tr><td>train/train_loss</td><td>0.9052</td></tr><tr><td>train/train_runtime</td><td>463.4362</td></tr><tr><td>train/train_samples_per_second</td><td>34.318</td></tr><tr><td>train/train_steps_per_second</td><td>2.149</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">xlmr-code-mixed-DS</strong>: <a href=\"https://wandb.ai/diptesh/aggression_detection/runs/2gjp1his\" target=\"_blank\">https://wandb.ai/diptesh/aggression_detection/runs/2gjp1his</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20220907_225508-2gjp1his/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "trainer = CustomTrainer(model_init=model_init,\n",
        "                        args=training_args,\n",
        "                        compute_metrics = compute_metrics,\n",
        "                        train_dataset = train_dataset,\n",
        "                        eval_dataset = valid_dataset,\n",
        "                        tokenizer = tokenizer, \n",
        "                        callbacks = [EarlyStoppingCallback(early_stopping_patience = 2, early_stopping_threshold=0.0001)]\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# post-training analysis, testing, other logged code\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "gguBpeh4xGdy"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to xlm-roberta-base-finetuned-code-mixed-DS\n",
            "Configuration saved in xlm-roberta-base-finetuned-code-mixed-DS/config.json\n",
            "Model weights saved in xlm-roberta-base-finetuned-code-mixed-DS/pytorch_model.bin\n",
            "tokenizer config file saved in xlm-roberta-base-finetuned-code-mixed-DS/tokenizer_config.json\n",
            "Special tokens file saved in xlm-roberta-base-finetuned-code-mixed-DS/special_tokens_map.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Text Classification', 'type': 'text-classification'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.6317907444668008}, {'name': 'Precision', 'type': 'precision', 'value': 0.578065000258759}, {'name': 'Recall', 'type': 'recall', 'value': 0.5977698793279365}, {'name': 'F1', 'type': 'f1', 'value': 0.5676661838384218}]}\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Several commits (2) will be pushed upstream.\n",
            "The progress bars may be unreliable.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "remote: Scanning LFS files for validity, may be slow...        \n",
            "remote: LFS file scan complete.        \n",
            "To https://huggingface.co/dipteshkanojia/xlm-roberta-base-finetuned-code-mixed-DS\n",
            "   c3e82d5..99e1c22  main -> main\n",
            "\n",
            "Upload file pytorch_model.bin: 100%|██████████| 1.04G/1.04G [05:33<00:00, 3.34MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        }
      ],
      "source": [
        "trainer.push_to_hub()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8DGegrFFB9c"
      },
      "source": [
        "## 8) Predictions and Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "uwWgMmrenkxF"
      },
      "outputs": [],
      "source": [
        "test_texts = list(test_df['Sentence'])\n",
        "test_labels = list(test_df['Label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "J18PaJADvljI"
      },
      "outputs": [],
      "source": [
        "test_encodings = tokenizer(test_texts, truncation=True, padding=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "aFnak-ItvrG-"
      },
      "outputs": [],
      "source": [
        "test_dataset = AggressionDataset(test_encodings, test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "qFi6MZz-g9xu"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Prediction *****\n",
            "  Num examples = 498\n",
            "  Batch size = 32\n",
            "/home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
          ]
        }
      ],
      "source": [
        "preds_output_test = trainer.predict(test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "nQJfQMYWhAtz"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'test_loss': 0.851223886013031,\n",
              " 'test_accuracy': 0.6365461847389559,\n",
              " 'test_precision': 0.5814707397216506,\n",
              " 'test_recall': 0.6010038979072099,\n",
              " 'test_f1': 0.5785081103966553,\n",
              " 'test_runtime': 2.0182,\n",
              " 'test_samples_per_second': 246.757,\n",
              " 'test_steps_per_second': 7.928}"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "preds_output_test.metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "tR_TsEhihCtm"
      },
      "outputs": [],
      "source": [
        "y_preds_test = np.argmax(preds_output_test.predictions, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "PfZhPHNFhE3B"
      },
      "outputs": [],
      "source": [
        "y_valid_test = np.array(test_dataset.labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "dZRj0AV4hGcP"
      },
      "outputs": [],
      "source": [
        "map_dt = {0:'NAG', 1:'CAG', 2:'OAG'}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "sgugEinyhH_X"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         NAG       0.89      0.75      0.82       268\n",
            "         CAG       0.45      0.39      0.42       136\n",
            "         OAG       0.41      0.66      0.50        94\n",
            "\n",
            "    accuracy                           0.64       498\n",
            "   macro avg       0.58      0.60      0.58       498\n",
            "weighted avg       0.68      0.64      0.65       498\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(classification_report(y_valid_test, y_preds_test, target_names=list(map_dt.values())))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "KCcc6g3NhLj7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Collecting seaborn\n",
            "  Downloading seaborn-0.12.0-py3-none-any.whl (285 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m285.1/285.1 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages (from seaborn) (1.21.6)\n",
            "Requirement already satisfied: pandas>=0.25 in /home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages (from seaborn) (1.3.5)\n",
            "Requirement already satisfied: matplotlib>=3.1 in /home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages (from seaborn) (3.2.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages (from matplotlib>=3.1->seaborn) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages (from matplotlib>=3.1->seaborn) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages (from matplotlib>=3.1->seaborn) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages (from matplotlib>=3.1->seaborn) (1.4.3)\n",
            "Requirement already satisfied: pytz>=2017.3 in /home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages (from pandas>=0.25->seaborn) (2019.3)\n",
            "Requirement already satisfied: six>=1.5 in /home/diptesh/anaconda3/envs/aggDet/lib/python3.8/site-packages (from python-dateutil>=2.1->matplotlib>=3.1->seaborn) (1.16.0)\n",
            "Installing collected packages: seaborn\n",
            "Successfully installed seaborn-0.12.0\n"
          ]
        }
      ],
      "source": [
        "!pip install seaborn\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "y_valid_trying = map(lambda x : map_dt[x], y_valid_test)\n",
        "y_valid_trying = list(y_valid_trying)\n",
        "\n",
        "y_preds_trying = map(lambda x : map_dt[x], y_preds_test)\n",
        "y_preds_trying = list(y_preds_trying)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "IwHUVo5PBE-x"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fdac8d4f340>"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "cm_labels = np.unique(y_valid_trying)\n",
        "cm_array = confusion_matrix(y_valid_trying, y_preds_trying)\n",
        "cm_array_df = pd.DataFrame(cm_array, index=cm_labels, columns=cm_labels)\n",
        "sns.heatmap(cm_array_df, annot=True, annot_kws={\"size\": 12}) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.8.13 ('aggDet')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "0c1cc14d24d579ea9f448eff41282ce5bee110707ee119424f8b85e664f18efb"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
